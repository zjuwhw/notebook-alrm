<!DOCTYPE html>
<html >

<head>

  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title></title>
  <meta name="description" content="">
  <meta name="generator" content="bookdown 0.5 and GitBook 2.6.7">

  <meta property="og:title" content="" />
  <meta property="og:type" content="book" />
  
  
  
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="" />
  
  
  




  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">
  
  
<link rel="prev" href="simultaneous-inferences-and-other-topics-in-regression-analysis.html">
<link rel="next" href="multiple-linear-regression-i.html">
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />









<style type="text/css">
div.sourceCode { overflow-x: auto; }
table.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {
  margin: 0; padding: 0; vertical-align: baseline; border: none; }
table.sourceCode { width: 100%; line-height: 100%; }
td.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; color: #aaaaaa; border-right: 1px solid #aaaaaa; }
td.sourceCode { padding-left: 5px; }
code > span.kw { color: #007020; font-weight: bold; } /* Keyword */
code > span.dt { color: #902000; } /* DataType */
code > span.dv { color: #40a070; } /* DecVal */
code > span.bn { color: #40a070; } /* BaseN */
code > span.fl { color: #40a070; } /* Float */
code > span.ch { color: #4070a0; } /* Char */
code > span.st { color: #4070a0; } /* String */
code > span.co { color: #60a0b0; font-style: italic; } /* Comment */
code > span.ot { color: #007020; } /* Other */
code > span.al { color: #ff0000; font-weight: bold; } /* Alert */
code > span.fu { color: #06287e; } /* Function */
code > span.er { color: #ff0000; font-weight: bold; } /* Error */
code > span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
code > span.cn { color: #880000; } /* Constant */
code > span.sc { color: #4070a0; } /* SpecialChar */
code > span.vs { color: #4070a0; } /* VerbatimString */
code > span.ss { color: #bb6688; } /* SpecialString */
code > span.im { } /* Import */
code > span.va { color: #19177c; } /* Variable */
code > span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code > span.op { color: #666666; } /* Operator */
code > span.bu { } /* BuiltIn */
code > span.ex { } /* Extension */
code > span.pp { color: #bc7a00; } /* Preprocessor */
code > span.at { color: #7d9029; } /* Attribute */
code > span.do { color: #ba2121; font-style: italic; } /* Documentation */
code > span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code > span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code > span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
</style>

</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Notebook for ALRM</a></li>

<li class="divider"></li>
<li class="chapter" data-level="1" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i><b>1</b> Preface</a></li>
<li class="chapter" data-level="2" data-path="linear-regression-with-one-predictor-variable.html"><a href="linear-regression-with-one-predictor-variable.html"><i class="fa fa-check"></i><b>2</b> Linear regression with one predictor variable</a><ul>
<li class="chapter" data-level="2.1" data-path="linear-regression-with-one-predictor-variable.html"><a href="linear-regression-with-one-predictor-variable.html#relations-between-variables"><i class="fa fa-check"></i><b>2.1</b> Relations between variables</a></li>
<li class="chapter" data-level="2.2" data-path="linear-regression-with-one-predictor-variable.html"><a href="linear-regression-with-one-predictor-variable.html#regression-models-and-their-uses"><i class="fa fa-check"></i><b>2.2</b> Regression Models and Their Uses</a></li>
<li class="chapter" data-level="2.3" data-path="linear-regression-with-one-predictor-variable.html"><a href="linear-regression-with-one-predictor-variable.html#simple-linear-regression-model-with-distribution-of-error-terms-unspecified"><i class="fa fa-check"></i><b>2.3</b> Simple linear regression model with distribution of error terms unspecified</a></li>
<li class="chapter" data-level="2.4" data-path="linear-regression-with-one-predictor-variable.html"><a href="linear-regression-with-one-predictor-variable.html#data-for-regression-analysis"><i class="fa fa-check"></i><b>2.4</b> Data for regression analysis</a></li>
<li class="chapter" data-level="2.5" data-path="linear-regression-with-one-predictor-variable.html"><a href="linear-regression-with-one-predictor-variable.html#overview-of-steps-in-regression-analysis"><i class="fa fa-check"></i><b>2.5</b> Overview of steps in regression analysis</a></li>
<li class="chapter" data-level="2.6" data-path="linear-regression-with-one-predictor-variable.html"><a href="linear-regression-with-one-predictor-variable.html#estimation-of-regression-function"><i class="fa fa-check"></i><b>2.6</b> Estimation of regression function</a></li>
<li class="chapter" data-level="2.7" data-path="linear-regression-with-one-predictor-variable.html"><a href="linear-regression-with-one-predictor-variable.html#estimation-of-erro-terms-variance-sigma2"><i class="fa fa-check"></i><b>2.7</b> Estimation of Erro Terms Variance <span class="math inline">\(\sigma^{2}\)</span></a></li>
<li class="chapter" data-level="2.8" data-path="linear-regression-with-one-predictor-variable.html"><a href="linear-regression-with-one-predictor-variable.html#normal-error-regression-model"><i class="fa fa-check"></i><b>2.8</b> Normal Error Regression Model</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="inferences-in-regeression-and-correlation-analysis.html"><a href="inferences-in-regeression-and-correlation-analysis.html"><i class="fa fa-check"></i><b>3</b> Inferences in Regeression and Correlation Analysis</a><ul>
<li class="chapter" data-level="3.1" data-path="inferences-in-regeression-and-correlation-analysis.html"><a href="inferences-in-regeression-and-correlation-analysis.html#inferences-concerning-beta_1"><i class="fa fa-check"></i><b>3.1</b> Inferences Concerning <span class="math inline">\(\beta_{1}\)</span></a></li>
<li class="chapter" data-level="3.2" data-path="inferences-in-regeression-and-correlation-analysis.html"><a href="inferences-in-regeression-and-correlation-analysis.html#inferences-concerning-beta_0"><i class="fa fa-check"></i><b>3.2</b> Inferences Concerning <span class="math inline">\(\beta_{0}\)</span></a></li>
<li class="chapter" data-level="3.3" data-path="inferences-in-regeression-and-correlation-analysis.html"><a href="inferences-in-regeression-and-correlation-analysis.html#some-considerations-on-making-inferences-concerning-beta_0-and-beta_1"><i class="fa fa-check"></i><b>3.3</b> Some Considerations on Making Inferences Concerning <span class="math inline">\(\beta_{0}\)</span> and <span class="math inline">\(\beta_{1}\)</span></a></li>
<li class="chapter" data-level="3.4" data-path="inferences-in-regeression-and-correlation-analysis.html"><a href="inferences-in-regeression-and-correlation-analysis.html#interval-estimation-of-ey_h"><i class="fa fa-check"></i><b>3.4</b> Interval Estimation of <span class="math inline">\(E(Y_{h})\)</span></a></li>
<li class="chapter" data-level="3.5" data-path="inferences-in-regeression-and-correlation-analysis.html"><a href="inferences-in-regeression-and-correlation-analysis.html#prediction-of-new-observation"><i class="fa fa-check"></i><b>3.5</b> Prediction of New Observation</a></li>
<li class="chapter" data-level="3.6" data-path="inferences-in-regeression-and-correlation-analysis.html"><a href="inferences-in-regeression-and-correlation-analysis.html#confidence-band-for-regression-line"><i class="fa fa-check"></i><b>3.6</b> Confidence Band for Regression Line</a></li>
<li class="chapter" data-level="3.7" data-path="inferences-in-regeression-and-correlation-analysis.html"><a href="inferences-in-regeression-and-correlation-analysis.html#analysis-of-variance-approach"><i class="fa fa-check"></i><b>3.7</b> Analysis of Variance Approach</a></li>
<li class="chapter" data-level="3.8" data-path="inferences-in-regeression-and-correlation-analysis.html"><a href="inferences-in-regeression-and-correlation-analysis.html#general-linear-test-approach"><i class="fa fa-check"></i><b>3.8</b> General Linear Test Approach</a></li>
<li class="chapter" data-level="3.9" data-path="inferences-in-regeression-and-correlation-analysis.html"><a href="inferences-in-regeression-and-correlation-analysis.html#descriptive-measures-of-linear-association-between-x-and-y"><i class="fa fa-check"></i><b>3.9</b> Descriptive Measures of Linear Association between X and Y</a></li>
<li class="chapter" data-level="3.10" data-path="inferences-in-regeression-and-correlation-analysis.html"><a href="inferences-in-regeression-and-correlation-analysis.html#considerations-in-applying-regression-analysis"><i class="fa fa-check"></i><b>3.10</b> Considerations in Applying Regression Analysis</a></li>
<li class="chapter" data-level="3.11" data-path="inferences-in-regeression-and-correlation-analysis.html"><a href="inferences-in-regeression-and-correlation-analysis.html#normal-correlation-models"><i class="fa fa-check"></i><b>3.11</b> Normal Correlation Models</a></li>
<li class="chapter" data-level="3.12" data-path="inferences-in-regeression-and-correlation-analysis.html"><a href="inferences-in-regeression-and-correlation-analysis.html#r-code"><i class="fa fa-check"></i><b>3.12</b> R code</a><ul>
<li class="chapter" data-level="3.12.1" data-path="inferences-in-regeression-and-correlation-analysis.html"><a href="inferences-in-regeression-and-correlation-analysis.html#example-data"><i class="fa fa-check"></i><b>3.12.1</b> Example data</a></li>
<li class="chapter" data-level="3.12.2" data-path="inferences-in-regeression-and-correlation-analysis.html"><a href="inferences-in-regeression-and-correlation-analysis.html#built-in-function"><i class="fa fa-check"></i><b>3.12.2</b> built-in function</a></li>
<li class="chapter" data-level="3.12.3" data-path="inferences-in-regeression-and-correlation-analysis.html"><a href="inferences-in-regeression-and-correlation-analysis.html#point-estimator-b_0-and-b_1"><i class="fa fa-check"></i><b>3.12.3</b> point estimator <span class="math inline">\(b_0\)</span> and <span class="math inline">\(b_1\)</span></a></li>
<li class="chapter" data-level="3.12.4" data-path="inferences-in-regeression-and-correlation-analysis.html"><a href="inferences-in-regeression-and-correlation-analysis.html#residuals-sse-and-mse"><i class="fa fa-check"></i><b>3.12.4</b> Residuals, SSE and MSE</a></li>
<li class="chapter" data-level="3.12.5" data-path="inferences-in-regeression-and-correlation-analysis.html"><a href="inferences-in-regeression-and-correlation-analysis.html#sampling-distribution-of-b_1-and-b_1beta_1sb_1"><i class="fa fa-check"></i><b>3.12.5</b> sampling distribution of <span class="math inline">\(b_1\)</span> and <span class="math inline">\((b_1−\beta_1)/s(b_1)\)</span></a></li>
<li class="chapter" data-level="3.12.6" data-path="inferences-in-regeression-and-correlation-analysis.html"><a href="inferences-in-regeression-and-correlation-analysis.html#f-test"><i class="fa fa-check"></i><b>3.12.6</b> F test</a></li>
<li class="chapter" data-level="3.12.7" data-path="inferences-in-regeression-and-correlation-analysis.html"><a href="inferences-in-regeression-and-correlation-analysis.html#r2-and-r"><i class="fa fa-check"></i><b>3.12.7</b> <span class="math inline">\(R^2\)</span> and r</a></li>
<li class="chapter" data-level="3.12.8" data-path="inferences-in-regeression-and-correlation-analysis.html"><a href="inferences-in-regeression-and-correlation-analysis.html#plot"><i class="fa fa-check"></i><b>3.12.8</b> plot</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="4" data-path="diagnostics-and-remedial-measures.html"><a href="diagnostics-and-remedial-measures.html"><i class="fa fa-check"></i><b>4</b> Diagnostics and Remedial Measures</a></li>
<li class="chapter" data-level="5" data-path="simultaneous-inferences-and-other-topics-in-regression-analysis.html"><a href="simultaneous-inferences-and-other-topics-in-regression-analysis.html"><i class="fa fa-check"></i><b>5</b> Simultaneous Inferences and Other Topics in Regression Analysis</a></li>
<li class="chapter" data-level="6" data-path="matrix-approach-to-simple-linear-regression-analysis.html"><a href="matrix-approach-to-simple-linear-regression-analysis.html"><i class="fa fa-check"></i><b>6</b> Matrix Approach to Simple Linear Regression Analysis</a><ul>
<li class="chapter" data-level="6.1" data-path="matrix-approach-to-simple-linear-regression-analysis.html"><a href="matrix-approach-to-simple-linear-regression-analysis.html#matrices"><i class="fa fa-check"></i><b>6.1</b> Matrices</a></li>
<li class="chapter" data-level="6.2" data-path="matrix-approach-to-simple-linear-regression-analysis.html"><a href="matrix-approach-to-simple-linear-regression-analysis.html#matrix-addition-and-subtraction"><i class="fa fa-check"></i><b>6.2</b> Matrix Addition and Subtraction</a></li>
<li class="chapter" data-level="6.3" data-path="matrix-approach-to-simple-linear-regression-analysis.html"><a href="matrix-approach-to-simple-linear-regression-analysis.html#matrix-multiplication"><i class="fa fa-check"></i><b>6.3</b> Matrix Multiplication</a></li>
<li class="chapter" data-level="6.4" data-path="matrix-approach-to-simple-linear-regression-analysis.html"><a href="matrix-approach-to-simple-linear-regression-analysis.html#special-types-of-matrices"><i class="fa fa-check"></i><b>6.4</b> Special Types of Matrices</a></li>
<li class="chapter" data-level="6.5" data-path="matrix-approach-to-simple-linear-regression-analysis.html"><a href="matrix-approach-to-simple-linear-regression-analysis.html#linear-dependence-and-rank-of-matrix"><i class="fa fa-check"></i><b>6.5</b> Linear Dependence and Rank of Matrix</a></li>
<li class="chapter" data-level="6.6" data-path="matrix-approach-to-simple-linear-regression-analysis.html"><a href="matrix-approach-to-simple-linear-regression-analysis.html#inverse-of-a-matrix"><i class="fa fa-check"></i><b>6.6</b> Inverse of a Matrix</a></li>
<li class="chapter" data-level="6.7" data-path="matrix-approach-to-simple-linear-regression-analysis.html"><a href="matrix-approach-to-simple-linear-regression-analysis.html#some-basic-results-for-matrics"><i class="fa fa-check"></i><b>6.7</b> Some Basic Results for Matrics</a></li>
<li class="chapter" data-level="6.8" data-path="matrix-approach-to-simple-linear-regression-analysis.html"><a href="matrix-approach-to-simple-linear-regression-analysis.html#random-vectors-and-matrices"><i class="fa fa-check"></i><b>6.8</b> Random Vectors and Matrices</a></li>
<li class="chapter" data-level="6.9" data-path="matrix-approach-to-simple-linear-regression-analysis.html"><a href="matrix-approach-to-simple-linear-regression-analysis.html#simple-linear-regression-model-in-matrix-terms"><i class="fa fa-check"></i><b>6.9</b> Simple Linear Regression Model in Matrix Terms</a></li>
<li class="chapter" data-level="6.10" data-path="matrix-approach-to-simple-linear-regression-analysis.html"><a href="matrix-approach-to-simple-linear-regression-analysis.html#leasst-squares-estimation-of-regression-parameters"><i class="fa fa-check"></i><b>6.10</b> Leasst Squares Estimation of Regression Parameters</a></li>
<li class="chapter" data-level="6.11" data-path="matrix-approach-to-simple-linear-regression-analysis.html"><a href="matrix-approach-to-simple-linear-regression-analysis.html#fitted-values-and-residuals"><i class="fa fa-check"></i><b>6.11</b> Fitted Values and Residuals</a></li>
<li class="chapter" data-level="6.12" data-path="matrix-approach-to-simple-linear-regression-analysis.html"><a href="matrix-approach-to-simple-linear-regression-analysis.html#analysis-of-variance-results"><i class="fa fa-check"></i><b>6.12</b> Analysis of Variance Results</a></li>
<li class="chapter" data-level="6.13" data-path="matrix-approach-to-simple-linear-regression-analysis.html"><a href="matrix-approach-to-simple-linear-regression-analysis.html#inferences-in-regeression-analysis"><i class="fa fa-check"></i><b>6.13</b> Inferences in Regeression Analysis</a></li>
<li class="chapter" data-level="6.14" data-path="matrix-approach-to-simple-linear-regression-analysis.html"><a href="matrix-approach-to-simple-linear-regression-analysis.html#r-code-1"><i class="fa fa-check"></i><b>6.14</b> R code</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="multiple-linear-regression-i.html"><a href="multiple-linear-regression-i.html"><i class="fa fa-check"></i><b>7</b> Multiple linear regression I</a><ul>
<li class="chapter" data-level="7.1" data-path="multiple-linear-regression-i.html"><a href="multiple-linear-regression-i.html#multiple-regression-models"><i class="fa fa-check"></i><b>7.1</b> Multiple Regression Models</a><ul>
<li class="chapter" data-level="7.1.1" data-path="multiple-linear-regression-i.html"><a href="multiple-linear-regression-i.html#first-order-model-with-two-predictor-variables"><i class="fa fa-check"></i><b>7.1.1</b> First-Order Model with Two Predictor Variables</a></li>
<li class="chapter" data-level="7.1.2" data-path="multiple-linear-regression-i.html"><a href="multiple-linear-regression-i.html#first-order-model-with-more-than-two-predictor-variables"><i class="fa fa-check"></i><b>7.1.2</b> First-Order Model with More than Two Predictor Variables</a></li>
<li class="chapter" data-level="7.1.3" data-path="multiple-linear-regression-i.html"><a href="multiple-linear-regression-i.html#general-linear-regression-model"><i class="fa fa-check"></i><b>7.1.3</b> General Linear Regression Model</a></li>
</ul></li>
<li class="chapter" data-level="7.2" data-path="multiple-linear-regression-i.html"><a href="multiple-linear-regression-i.html#general-linear-regression-model-in-matrix-terms"><i class="fa fa-check"></i><b>7.2</b> General Linear Regression Model in Matrix Terms</a></li>
<li class="chapter" data-level="7.3" data-path="multiple-linear-regression-i.html"><a href="multiple-linear-regression-i.html#estimation-of-regression-coefficients"><i class="fa fa-check"></i><b>7.3</b> Estimation of Regression Coefficients</a></li>
<li class="chapter" data-level="7.4" data-path="multiple-linear-regression-i.html"><a href="multiple-linear-regression-i.html#fitted-values-and-residuals-1"><i class="fa fa-check"></i><b>7.4</b> Fitted Values and Residuals</a></li>
<li class="chapter" data-level="7.5" data-path="multiple-linear-regression-i.html"><a href="multiple-linear-regression-i.html#analysis-of-variance-results-1"><i class="fa fa-check"></i><b>7.5</b> Analysis of Variance Results</a></li>
<li class="chapter" data-level="7.6" data-path="multiple-linear-regression-i.html"><a href="multiple-linear-regression-i.html#inferences-about-regression-parameters"><i class="fa fa-check"></i><b>7.6</b> Inferences about Regression Parameters</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="multiple-regression-ii.html"><a href="multiple-regression-ii.html"><i class="fa fa-check"></i><b>8</b> Multiple Regression II</a><ul>
<li class="chapter" data-level="8.1" data-path="multiple-regression-ii.html"><a href="multiple-regression-ii.html#extra-sums-of-squares"><i class="fa fa-check"></i><b>8.1</b> Extra Sums of Squares</a></li>
</ul></li>
<li class="divider"></li>
Published with <a href="https://github.com/rstudio/bookdown" target="blank">bookdown</a> by <a href="http://zjuwhw.github.io/" target="blank">zjuwhw</a>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./"></a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="matrix-approach-to-simple-linear-regression-analysis" class="section level1">
<h1><span class="header-section-number">6</span> Matrix Approach to Simple Linear Regression Analysis</h1>
<div id="matrices" class="section level2">
<h2><span class="header-section-number">6.1</span> Matrices</h2>
<ul>
<li>Definition
<ul>
<li>matrix</li>
<li>elements</li>
<li>dimension</li>
</ul></li>
<li>Notation: a boldface symbol, such as <strong>A</strong>, <strong>X</strong> or <strong>Z</strong>.</li>
<li><strong>Square Matrix</strong>: the number of rows equals the number of columns</li>
<li><strong>Vector</strong>
<ul>
<li>column vector/vector: only one column matrix</li>
<li>row vector</li>
</ul></li>
<li><strong>Transpose</strong>: <strong>A’</strong> (A prime) is the transpose of a matrix <strong>A</strong></li>
<li><strong>Equality of Matrices</strong>: same dimension and all same corresponding elements</li>
<li><strong>design matrix</strong></li>
</ul>
</div>
<div id="matrix-addition-and-subtraction" class="section level2">
<h2><span class="header-section-number">6.2</span> Matrix Addition and Subtraction</h2>
<ul>
<li>same dimension</li>
<li>the sum or difference of the corresponding elements of the two matrixs</li>
<li><strong>A</strong> + <strong>B</strong> = <strong>B</strong> + <strong>A</strong></li>
</ul>
</div>
<div id="matrix-multiplication" class="section level2">
<h2><span class="header-section-number">6.3</span> Matrix Multiplication</h2>
<ul>
<li>Multiplication of a Matrix by a <strong>scalar</strong>
<ul>
<li>a scalar is an ordinary number or a symbol representing a number</li>
</ul></li>
<li>Multiplication of a Matrix by a Matrix
<ul>
<li>the product <strong>AB</strong>, we say that <strong>A</strong> is postmultiplied by <strong>B</strong> or <strong>B</strong> is premultiplied by <strong>A</strong></li>
<li><span class="math inline">\(AB \neq BA\)</span></li>
</ul></li>
</ul>
<p>In general, if <strong>A</strong> has dimension r * c and <strong>B</strong> has dimension c * s, the product <strong>AB</strong> is a matrix of dimension r * s, which is</p>
<p><span class="math display">\[AB_{r \times s} = \begin{bmatrix}\sum_{k=1}^{c} a_{ik}b_{kj}\end{bmatrix}\text{, where }i=1,...,r;j=1,...,s\]</span></p>
<ul>
<li>Regression Examples</li>
</ul>
<p><span class="math display">\[Y&#39;Y_{1 \times 1} = \begin{bmatrix} Y_{1} &amp; Y_{2} &amp; ... &amp; Y_{n} \end{bmatrix}\begin{bmatrix}Y_{1} \\ Y_{2} \\ ... \\ Y_{n} \end{bmatrix} =  Y_{1}^{2} + Y_{2}^{2} + ... + Y_{n}^{2} = \sum Y_{i}^{2}\]</span></p>
<p><span class="math display">\[X&#39;X_{2 \times 2} = \begin{bmatrix} 1 &amp; 1 &amp; ... &amp; 1 \\ X_{1} &amp; X_{2} &amp; ... &amp; X_{n} \end{bmatrix}\begin{bmatrix} 1 &amp; X_{1} \\ 1 &amp; X_{2} \\ ... \\ 1 &amp; X_{n} \end{bmatrix} =  \begin{bmatrix} n &amp; \sum X_{i} \\ \sum X_{i} &amp; \sum X_{i}^{2} \end{bmatrix}\]</span></p>
<p><span class="math display">\[X&#39;Y_{2 \times 1} = \begin{bmatrix} 1 &amp; 1 &amp; ... &amp; 1 \\ X_{1} &amp; X_{2} &amp; ... &amp; X_{n} \end{bmatrix}\begin{bmatrix} Y_{1} \\ Y_{2} \\ ... \\ Y_{n} \end{bmatrix} =  \begin{bmatrix} \sum Y_{i} \\ \sum X_{i}Y_{i} \end{bmatrix}\]</span></p>
</div>
<div id="special-types-of-matrices" class="section level2">
<h2><span class="header-section-number">6.4</span> Special Types of Matrices</h2>
<ul>
<li><strong>Symmetric Matrix</strong>: <strong>A</strong> = <strong>A’</strong>
<ul>
<li>Symmetric matrix is necessarily square</li>
<li>premultiply a matrix by its transpose, say <strong>X’X</strong> is symmetric</li>
</ul></li>
<li><strong>Diagonal Matrix</strong>: off-diagonal elements are all zeros</li>
<li><strong>Identity Matrix</strong>, denoted by <strong>I</strong>: a diagonal matrix whose elements on the main diagonal are all 1s.
<ul>
<li><strong>AI</strong> = <strong>IA</strong> = <strong>A</strong>, <span class="math inline">\(A, I \in \mathbb{R}^{r \times r}\)</span></li>
</ul></li>
<li><strong>Scalar Matrix</strong>: a diagonal matrix whose main-diagonal elements are the same, can be expressed as k<strong>I</strong></li>
<li>Vector and matrix with all elements unity
<ul>
<li>a column vector with all elements 1 will be denoted by <strong>1</strong></li>
<li>a square matrix with all elements 1 will be denoted by <strong>J</strong></li>
<li><strong>1’1</strong> = n</li>
<li><strong>11’</strong> = <strong>J</strong></li>
</ul></li>
<li>Zero Vector: a vector containing only zeros, denoted by <strong>0</strong></li>
</ul>
</div>
<div id="linear-dependence-and-rank-of-matrix" class="section level2">
<h2><span class="header-section-number">6.5</span> Linear Dependence and Rank of Matrix</h2>
<ul>
<li>Linear dependence</li>
</ul>
<p>We define the set of c column vectors <span class="math inline">\(C_{1}, ..., C_{c}\)</span> in an r <span class="math inline">\(\times\)</span> c matrix to be <strong>linearly dependent</strong> if one vector can be expressed as a linear combination of others. If no vector in the set can be so expressed, we define the set of vectors to be <strong>linearly independent</strong>.</p>
<p>In a more general, when c scalars <span class="math inline">\(k_{1},...,k_{c}\)</span>, not all zero, can be found such that:</p>
<p><span class="math display">\[k_{1}\mathbf{C_{1}} + k_{2}\mathbf{C_{2}} + ... + k_{c}\mathbf{C_{c}} = \mathbf{0}\]</span></p>
<p>where <strong>0</strong> denotes the zero column vector, <strong>the c column vectors</strong> are <strong>linearly dependent</strong>. If the only set of scalars for which the equality holds is <span class="math inline">\(k_{1} = k_{2} = ... = k_{c} = 0\)</span>, <strong>the set of c column vectors</strong> is <strong>linearly independent</strong>.</p>
<ul>
<li>Rank of Matrix: the maximum number of linearly independent columns in the matrix
<ul>
<li>the rank of r <span class="math inline">\(\times\)</span> c matrix cannot exceed min(r, c)</li>
<li>When a matrix is the product of two matrixs, its rank cannot exceed the smaller of the two ranks for the matrices being multiplied.</li>
</ul></li>
</ul>
</div>
<div id="inverse-of-a-matrix" class="section level2">
<h2><span class="header-section-number">6.6</span> Inverse of a Matrix</h2>
<p>The inverse of a matrix <span class="math inline">\(\mathbf{A}\)</span> is another matrix, denoted by <span class="math inline">\(\mathbf{A^{-1}}\)</span>, such that:</p>
<p><span class="math display">\[\mathbf{A}^{-1}\mathbf{A} = \mathbf{AA}^{-1} = \mathbf{I}\]</span></p>
<p>where <strong>I</strong> is the identity matrix.</p>
<ul>
<li>the inverse of a matrix is defined only for square matrix</li>
<li>many square matrix do not have inverse</li>
<li>the inverse of a square matrix, if exits, is unique</li>
</ul>
<p>Finding the inverse</p>
<ul>
<li>An inverse of a square <span class="math inline">\(r \times r\)</span> matrix exists if the rank of the matrix is r. Such a matrix is said to be <strong>nonsingular</strong> or of <strong>full rank</strong>.</li>
<li>An <span class="math inline">\(r \times r\)</span> matrix with rank less than r is said to be <strong>singular</strong> or <strong>not of full rank</strong>, and does not have an inverse.</li>
<li>The inverse of an <span class="math inline">\(r \times r\)</span> matrix of full rank also has rank r.</li>
</ul>
<p>If:</p>
<p><span class="math display">\[\mathbf{A}_{2 \times 2} = \begin{bmatrix} a &amp; b \\ c &amp; d \end{bmatrix}\]</span></p>
<p>then:</p>
<p><span class="math display">\[\mathbf{A}_{2 \times 2}^{-1} = \begin{bmatrix} \frac{d}{D} &amp; \frac{-b}{D} \\ \frac{-c}{D} &amp; \frac{a}{D} \end{bmatrix}\]</span></p>
<p>where:</p>
<p><span class="math display">\[D = ad - bc\]</span></p>
<p>D is called the <strong>determinant</strong>(行列式) of the matrix <strong>A</strong>. If <strong>A</strong> were singular, its determinant would equal zero and no inverse of A would exist.</p>
<p>Regression Example</p>
<p><span class="math display">\[\mathbf{X}&#39;\mathbf{X}_{2 \times 2} = \begin{bmatrix} n &amp; \sum X_{i} \\ \sum X_{i} &amp; \sum X_{i}^{2} \end{bmatrix}\]</span></p>
<p><span class="math display">\[ a = n, b = c = \sum{X_{i}}, d = \sum{X_{i}^{2}} \]</span></p>
<p><span class="math display">\[ D = n\sum{X_{i}^{2}} - (\sum{X_{i}})^{2} = n\sum{(X_{i} - \bar{X})}^{2}\]</span></p>
<p><span class="math display">\[(\mathbf{X}&#39;\mathbf{X})_{2 \times 2}^{-1} = \begin{bmatrix} \frac{1}{n} + \frac{\bar{X}^2}{\sum(X_{i} - \bar{X})^{2}} &amp; \frac{-\bar{X}}{\sum{(X_{i} - \bar{X})^2}} \\ \frac{-\bar{X}}{\sum{(X_{i} - \bar{X})^2}} &amp; \frac{1}{\sum{(X_{i} - \bar{X})^2}} \end{bmatrix}\]</span></p>
</div>
<div id="some-basic-results-for-matrics" class="section level2">
<h2><span class="header-section-number">6.7</span> Some Basic Results for Matrics</h2>
<ul>
<li>A + B = B + A</li>
<li>(A + B) + C = A + (B + C)</li>
<li>(AB)C = A(BC)</li>
<li>C(A + B) = CA + CB</li>
<li>k(A + B) = kA + kB</li>
<li>(A’)’ = A</li>
<li>(A + B)‘= A’ + B’</li>
<li>(AB)‘= B’A’</li>
<li>(ABC)‘= C’B’A’</li>
<li><span class="math inline">\((AB)^{-1} = B^{-1}A^{-1}\)</span></li>
<li><span class="math inline">\((ABC)^{-1} = C^{-1}B^{-1}A^{-1}\)</span></li>
<li><span class="math inline">\((A^{-1})^{-1} = A\)</span></li>
<li><span class="math inline">\((A&#39;)^{-1} = (A^{-1})&#39;\)</span></li>
</ul>
</div>
<div id="random-vectors-and-matrices" class="section level2">
<h2><span class="header-section-number">6.8</span> Random Vectors and Matrices</h2>
<ul>
<li><strong>A random vector or matrix</strong> contains elements that are random variables.</li>
<li><strong>Expectation</strong> of random vector or matrix:</li>
</ul>
<p><span class="math display">\[\mathbf{Y}_{3 \times 1} = \begin{bmatrix} Y_1 \\ Y_2 \\ Y_3 \end{bmatrix}\text{, and } \mathbf{E(Y)}_{3 \times 1 } = \begin{bmatrix} E(Y_1) \\ E(Y_2) \\ E(Y_3) \end{bmatrix}\]</span></p>
<ul>
<li><strong>Variance-Covariance Matrix of Random Vector</strong></li>
</ul>
<p><span class="math display">\[\sigma^2(\mathbf{Y})_{n \times n} = \begin{bmatrix} \sigma^2(Y_1) &amp; \sigma(Y_1,Y_2) &amp; ... &amp; \sigma(Y_1, Y_n) \\  \sigma(Y_2,Y_1) &amp; \sigma^2(Y_2) &amp; ... &amp; \sigma(Y_2, Y_n) \\ ... &amp; ... &amp; ... &amp; ...\\  \sigma(Y_n, Y_1) &amp;  \sigma(Y_n, Y_2) &amp; ... &amp; \sigma^2(Y_n) \end{bmatrix}\]</span></p>
<p>which is a symmetric matrix.</p>
<ul>
<li>Some Basic Rules</li>
</ul>
<p><span class="math display">\[\mathbf{W} = \mathbf{AY}\]</span>,</p>
<p>which <strong>W</strong> and <strong>Y</strong> are two random vectors and <strong>A</strong> is a constant matrix</p>
<p><span class="math display">\[E(\mathbf{A}) = \mathbf{A}\]</span></p>
<p><span class="math display">\[E(\mathbf{W}) = E(\mathbf{AY}) = \mathbf{A}E(\mathbf{Y})\]</span></p>
<p><span class="math display">\[\sigma^2(\mathbf{W}) = \sigma^2(\mathbf{AY}) = \mathbf{A}\sigma^2(\mathbf{Y})\mathbf{A&#39;}\]</span></p>
<ul>
<li>Multivariate Normal Distribution</li>
</ul>
</div>
<div id="simple-linear-regression-model-in-matrix-terms" class="section level2">
<h2><span class="header-section-number">6.9</span> Simple Linear Regression Model in Matrix Terms</h2>
<p>The normal error regression model in matrix terms is:</p>
<p><span class="math display">\[\underset{n \times 1}{\mathbf{Y}} = \underset{n \times 2}{\mathbf{X}}\underset{2 \times 1}{\boldsymbol{\beta}} + \underset{n \times 1}{\boldsymbol{\varepsilon}}\]</span> , where</p>
<p><span class="math display">\[\underset{n \times 1}{\mathbf{Y}} = \begin{bmatrix}y_1 \\ y_2 \\ ... \\y_n\end{bmatrix}, \underset{n \times 2}{\mathbf{X}} = \begin{bmatrix}1 &amp; x_1 \\ 1 &amp; x_2 \\ ...  &amp; ... \\ 1  &amp; x_n \end{bmatrix}, \underset{2 \times 1}{\boldsymbol{\beta}} = \begin{bmatrix}\beta_0 \\ \beta_1 \end{bmatrix} and \underset{n \times 1}{\boldsymbol{\varepsilon}} = \begin{bmatrix}\varepsilon_1 \\ \varepsilon_2 \\ ... \\\varepsilon_n\end{bmatrix}\]</span></p>
</div>
<div id="leasst-squares-estimation-of-regression-parameters" class="section level2">
<h2><span class="header-section-number">6.10</span> Leasst Squares Estimation of Regression Parameters</h2>
<p><span class="math display">\[\mathbf{b} = (\mathbf{X}&#39;\mathbf{X})^{-1}\mathbf{X}&#39;\mathbf{Y},\]</span></p>
<p>where <strong>b</strong> is the vector of the least squares regression coefficients:</p>
<p><span class="math display">\[ \mathbf{b} = \begin{bmatrix} b_0 \\ b_1 \end{bmatrix} \]</span></p>
<p>Note that, the inverse is only valid for square matrix and <span class="math inline">\(\mathbf{X}&#39;\mathbf{X}\)</span> is definitely a square matrix.</p>
</div>
<div id="fitted-values-and-residuals" class="section level2">
<h2><span class="header-section-number">6.11</span> Fitted Values and Residuals</h2>
<p><span class="math display">\[\hat{\mathbf{Y}} = \mathbf{X}\mathbf{b} = \mathbf{X}(\mathbf{X}&#39;\mathbf{X})^{-1}\mathbf{X}&#39;\mathbf{Y}\]</span></p>
<p>or</p>
<p><span class="math display">\[\hat{\mathbf{Y}} = \mathbf{H}\mathbf{Y}\text{, with }\underset{n \times n}{\mathbf{H}} = \mathbf{X}(\mathbf{X}&#39;\mathbf{X})^{-1}\mathbf{X}&#39;\]</span></p>
<p>The matrix <strong>H</strong> is called <strong>hat matrix</strong>. And it’s symmetric and has the special property (called idempotency):</p>
<p><span class="math display">\[\mathbf{HH} = \mathbf{H}\]</span></p>
<p><span class="math display">\[\mathbf{e} = \mathbf{Y} - \mathbf{\hat{Y}} = \mathbf{Y} - \mathbf{HY} = (\mathbf{I}-\mathbf{H})\mathbf{Y}\]</span></p>
<p>and the matrix <span class="math inline">\(\mathbf{I}-\mathbf{H}\)</span>, like the matrix <span class="math inline">\(\mathbf{H}\)</span>, is symmetric and idempotent.</p>
<p>The variance-covariance matrix of residuals <strong>e</strong>:</p>
<p><span class="math display">\[\sigma^2(\mathbf{e}) = \sigma^2 \times (\mathbf{I}-\mathbf{H})\]</span></p>
<p>and is estimated by:</p>
<p><span class="math display">\[s^2(\mathbf{3}) = MSE \times (\mathbf{I}-\mathbf{H})\]</span></p>
<ul>
<li>Proof:</li>
</ul>
<p><span class="math display">\[\sigma^2(\mathbf{e}) = \sigma^2((\mathbf{I}-\mathbf{H})\mathbf{Y}) = (\mathbf{I}-\mathbf{H})\times \sigma^2(\mathbf{Y}) \times (\mathbf{I}-\mathbf{H})&#39;\]</span></p>
<p><span class="math display">\[\sigma^2(\mathbf{Y})= \sigma^2 \times \mathbf{I}\]</span></p>
<p><span class="math display">\[(\mathbf{I}-\mathbf{H})&#39; = (\mathbf{I}-\mathbf{H})\]</span></p>
<p><span class="math display">\[(\mathbf{I}-\mathbf{H})(\mathbf{I}-\mathbf{H}) = \mathbf{I}-\mathbf{H}\]</span></p>
<p><span class="math display">\[\sigma^2(\mathbf{e}) = \sigma^2 \times (\mathbf{I}-\mathbf{H})\]</span></p>
</div>
<div id="analysis-of-variance-results" class="section level2">
<h2><span class="header-section-number">6.12</span> Analysis of Variance Results</h2>
<p><span class="math display">\[SSTO = \sum(Y_i - \bar{Y})^2 = \sum Y_i^2 - \frac{(\sum Y_i)^2}{n} = \mathbf{Y}&#39;\mathbf{Y} - (\frac{1}{n})\mathbf{Y}&#39;\mathbf{JY}\]</span></p>
<p><span class="math display">\[SSE = \mathbf{e}&#39;\mathbf{e} = (\mathbf{Y}-\mathbf{Xb})&#39;(\mathbf{Y}-\mathbf{Xb})=\mathbf{Y}&#39;\mathbf{Y} - \mathbf{b}&#39;\mathbf{X}&#39;\mathbf{Y}\]</span></p>
<p><span class="math display">\[SSR = \mathbf{b}&#39;\mathbf{X}&#39;\mathbf{Y} - (\frac{1}{n})\mathbf{Y}&#39;\mathbf{JY}\]</span> A quadratic form is defined as:</p>
<p><span class="math display">\[\underset{1 \times 1}{\mathbf{Y}&#39;\mathbf{AY}} = \displaystyle\sum_{i=1}^{n}\displaystyle\sum_{j=1}^{n}a_{ij}Y_iY_j\text{, where }a_{ij} = a_{ji}\]</span></p>
<p><strong>A</strong> is a symmetrc n by n matrix and is called <strong>the matrix of the quadratic form</strong>.</p>
<p>So the sums of squares as quadratic forms as follows:</p>
<p><span class="math display">\[SSTO = \mathbf{Y}&#39;[\mathbf{I} - \frac{1}{n}\mathbf{J}]\mathbf{Y}\]</span></p>
<p><span class="math display">\[SSE = \mathbf{Y}&#39;[\mathbf{I} - \mathbf{H}]\mathbf{Y}\]</span></p>
<p><span class="math display">\[SSR = \mathbf{Y}&#39;[\mathbf{H} - \frac{1}{n}\mathbf{J}]\mathbf{Y} \]</span></p>
<p>Quadratic forms play an important role in statistics because all sums of squares in the analysis of variance for linear statistical models can be expressed as quadratic forms.</p>
</div>
<div id="inferences-in-regeression-analysis" class="section level2">
<h2><span class="header-section-number">6.13</span> Inferences in Regeression Analysis</h2>
<ul>
<li>Regression Coefficients</li>
</ul>
<p>The variance-covariance matrix of <strong>b</strong>:</p>
<p><span class="math display">\[\sigma^2(\mathbf{b}) = \begin{bmatrix} \sigma^2(b_0) &amp; \sigma(b_0,b_1) \\ \sigma(b_0,b_1) &amp; \sigma^2(b_1) \end{bmatrix} = \sigma^2 \times (\mathbf{X}&#39;\mathbf{X})^{-1} = \begin{bmatrix} \frac{\sigma^2}{n} + \frac{\sigma^2\bar{X}^2}{\sum(X_i - \bar{X})^2} &amp; \frac{-\bar{X}\sigma^2}{\sum(X_i - \bar{X})^2} \\ \frac{-\bar{X}\sigma^2}{\sum(X_i - \bar{X})^2} &amp; \frac{\sigma^2}{\sum(X_i - \bar{X})^2} \end{bmatrix}\]</span></p>
<p>And the estimated variance-covariance matrix of <strong>b</strong>, denoted by <span class="math inline">\(s^2(\mathbf{b})\)</span>:</p>
<p><span class="math display">\[s^2(\mathbf{b}) = MSE \times  (\mathbf{X}&#39;\mathbf{X})^{-1}\]</span></p>
<ul>
<li>Mean Response</li>
</ul>
<p><span class="math display">\[s^2(\hat{Y}_h) = MSE \times (\mathbf{X}_{h}&#39;(\mathbf{X}&#39;\mathbf{X})^{-1}\mathbf{X}_h) = MSE \times [\frac{1}{n} + \frac{(X_h - \bar{X})^2}{\sum(X_i - \bar{X})^2}]\]</span></p>
<ul>
<li>Prediction of new observation</li>
</ul>
<p><span class="math display">\[s^2(pred) = MSE \times (1+\mathbf{X}_{h}&#39;(\mathbf{X}&#39;\mathbf{X})^{-1}\mathbf{X}_h)\]</span></p>
<ul>
<li>Proof:</li>
</ul>
<p><span class="math display">\[\sigma^2(\mathbf{b}) = \sigma^2(\mathbf{(X&#39;X)^{-1}X&#39;Y}) = \mathbf{(X&#39;X)^{-1}X&#39;}\sigma^2(\mathbf{Y})(\mathbf{(X&#39;X)^{-1}X&#39;})&#39; = \sigma^2 \times (\mathbf{X}&#39;\mathbf{X})^{-1}\]</span></p>
</div>
<div id="r-code-1" class="section level2">
<h2><span class="header-section-number">6.14</span> R code</h2>
<ul>
<li>example data</li>
</ul>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">head</span>(trees)</code></pre></div>
<pre><code>##   Girth Height Volume
## 1   8.3     70   10.3
## 2   8.6     65   10.3
## 3   8.8     63   10.2
## 4  10.5     72   16.4
## 5  10.7     81   18.8
## 6  10.8     83   19.7</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">fit_lm =<span class="st"> </span><span class="kw">lm</span>(trees<span class="op">$</span><span class="st">&quot;Girth&quot;</span> <span class="op">~</span><span class="st"> </span>trees<span class="op">$</span><span class="st">&quot;Height&quot;</span>)
<span class="kw">summary</span>(fit_lm)</code></pre></div>
<pre><code>## 
## Call:
## lm(formula = trees$Girth ~ trees$Height)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -4.2386 -1.9205 -0.0714  2.7450  4.5384 
## 
## Coefficients:
##              Estimate Std. Error t value Pr(&gt;|t|)   
## (Intercept)  -6.18839    5.96020  -1.038  0.30772   
## trees$Height  0.25575    0.07816   3.272  0.00276 **
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 2.728 on 29 degrees of freedom
## Multiple R-squared:  0.2697, Adjusted R-squared:  0.2445 
## F-statistic: 10.71 on 1 and 29 DF,  p-value: 0.002758</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">fitted</span>(fit_lm)</code></pre></div>
<pre><code>##         1         2         3         4         5         6         7 
## 11.713904 10.435169  9.923674 12.225399 14.527123 15.038617 10.690916 
##         8         9        10        11        12        13        14 
## 12.992640 14.271376 12.992640 14.015628 13.248387 13.248387 11.458157 
##        15        16        17        18        19        20        21 
## 12.992640 12.736893 15.550111 15.805858 11.969651 10.179422 13.759881 
##        22        23        24        25        26        27        28 
## 14.271376 12.736893 12.225399 13.504134 14.527123 14.782870 14.271376 
##        29        30        31 
## 14.271376 14.271376 16.061605</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">anova</span>(fit_lm)</code></pre></div>
<pre><code>## Analysis of Variance Table
## 
## Response: trees$Girth
##              Df  Sum Sq Mean Sq F value   Pr(&gt;F)   
## trees$Height  1  79.665  79.665  10.707 0.002758 **
## Residuals    29 215.772   7.440                    
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</code></pre>
<ul>
<li>Least squaress estimation</li>
</ul>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">y =<span class="st"> </span>trees<span class="op">$</span><span class="st">&quot;Girth&quot;</span>
x =<span class="st"> </span><span class="kw">cbind</span>(<span class="dv">1</span>,trees<span class="op">$</span><span class="st">&quot;Height&quot;</span>)
b =<span class="st"> </span><span class="kw">solve</span>(<span class="kw">t</span>(x) <span class="op">%*%</span><span class="st"> </span>x) <span class="op">%*%</span><span class="st"> </span><span class="kw">t</span>(x) <span class="op">%*%</span><span class="st"> </span>y
b</code></pre></div>
<pre><code>##            [,1]
## [1,] -6.1883945
## [2,]  0.2557471</code></pre>
<ul>
<li>fitted values</li>
</ul>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">h =<span class="st"> </span>x <span class="op">%*%</span><span class="st"> </span><span class="kw">solve</span>(<span class="kw">t</span>(x) <span class="op">%*%</span><span class="st"> </span>x) <span class="op">%*%</span><span class="st"> </span><span class="kw">t</span>(x)
<span class="kw">dim</span>(h); h[<span class="dv">1</span><span class="op">:</span><span class="dv">4</span>,<span class="dv">1</span><span class="op">:</span><span class="dv">4</span>]</code></pre></div>
<pre><code>## [1] 31 31</code></pre>
<pre><code>##            [,1]       [,2]       [,3]       [,4]
## [1,] 0.06181471 0.08644526 0.09629747 0.05196250
## [2,] 0.08644526 0.13160125 0.14966365 0.06838286
## [3,] 0.09629747 0.14966365 0.17101012 0.07495100
## [4,] 0.05196250 0.06838286 0.07495100 0.04539435</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">yhat =<span class="st"> </span>h <span class="op">%*%</span><span class="st"> </span>y
<span class="kw">head</span>(yhat)</code></pre></div>
<pre><code>##           [,1]
## [1,] 11.713904
## [2,] 10.435169
## [3,]  9.923674
## [4,] 12.225399
## [5,] 14.527123
## [6,] 15.038617</code></pre>
<ul>
<li>sum of squares</li>
</ul>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">SSTO =<span class="st"> </span><span class="kw">t</span>(y) <span class="op">%*%</span><span class="st"> </span>y <span class="op">-</span><span class="st"> </span><span class="dv">1</span> <span class="op">/</span><span class="st"> </span><span class="kw">length</span>(y) <span class="op">*</span><span class="st"> </span><span class="kw">t</span>(y) <span class="op">%*%</span><span class="st"> </span><span class="kw">matrix</span>(<span class="dv">1</span>, <span class="dt">nrow=</span><span class="kw">length</span>(y), <span class="dt">ncol=</span><span class="kw">length</span>(y)) <span class="op">%*%</span><span class="st"> </span>y
SSTO</code></pre></div>
<pre><code>##          [,1]
## [1,] 295.4374</code></pre>

</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="simultaneous-inferences-and-other-topics-in-regression-analysis.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="multiple-linear-regression-i.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"google": false,
"weibo": false,
"instapper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"download": null,
"toc": {
"collapse": "none"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://cdn.bootcss.com/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:" && /^https?:/.test(script.src))
      script.src  = script.src.replace(/^https?:/, '');
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
