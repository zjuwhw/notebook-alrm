<!DOCTYPE html>
<html >

<head>

  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title></title>
  <meta name="description" content="">
  <meta name="generator" content="bookdown 0.5 and GitBook 2.6.7">

  <meta property="og:title" content="" />
  <meta property="og:type" content="book" />
  
  
  
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="" />
  
  
  




  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">
  
  
<link rel="prev" href="index.html">
<link rel="next" href="inferences-in-regeression-and-correlation-analysis.html">
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />









<style type="text/css">
div.sourceCode { overflow-x: auto; }
table.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {
  margin: 0; padding: 0; vertical-align: baseline; border: none; }
table.sourceCode { width: 100%; line-height: 100%; }
td.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; color: #aaaaaa; border-right: 1px solid #aaaaaa; }
td.sourceCode { padding-left: 5px; }
code > span.kw { color: #007020; font-weight: bold; } /* Keyword */
code > span.dt { color: #902000; } /* DataType */
code > span.dv { color: #40a070; } /* DecVal */
code > span.bn { color: #40a070; } /* BaseN */
code > span.fl { color: #40a070; } /* Float */
code > span.ch { color: #4070a0; } /* Char */
code > span.st { color: #4070a0; } /* String */
code > span.co { color: #60a0b0; font-style: italic; } /* Comment */
code > span.ot { color: #007020; } /* Other */
code > span.al { color: #ff0000; font-weight: bold; } /* Alert */
code > span.fu { color: #06287e; } /* Function */
code > span.er { color: #ff0000; font-weight: bold; } /* Error */
code > span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
code > span.cn { color: #880000; } /* Constant */
code > span.sc { color: #4070a0; } /* SpecialChar */
code > span.vs { color: #4070a0; } /* VerbatimString */
code > span.ss { color: #bb6688; } /* SpecialString */
code > span.im { } /* Import */
code > span.va { color: #19177c; } /* Variable */
code > span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code > span.op { color: #666666; } /* Operator */
code > span.bu { } /* BuiltIn */
code > span.ex { } /* Extension */
code > span.pp { color: #bc7a00; } /* Preprocessor */
code > span.at { color: #7d9029; } /* Attribute */
code > span.do { color: #ba2121; font-style: italic; } /* Documentation */
code > span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code > span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code > span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
</style>

</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Notebook for ALRM</a></li>

<li class="divider"></li>
<li class="chapter" data-level="1" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i><b>1</b> Preface</a></li>
<li class="chapter" data-level="2" data-path="linear-regression-with-one-predictor-variable.html"><a href="linear-regression-with-one-predictor-variable.html"><i class="fa fa-check"></i><b>2</b> Linear regression with one predictor variable</a><ul>
<li class="chapter" data-level="2.1" data-path="linear-regression-with-one-predictor-variable.html"><a href="linear-regression-with-one-predictor-variable.html#relations-between-variables"><i class="fa fa-check"></i><b>2.1</b> Relations between variables</a></li>
<li class="chapter" data-level="2.2" data-path="linear-regression-with-one-predictor-variable.html"><a href="linear-regression-with-one-predictor-variable.html#regression-models-and-their-uses"><i class="fa fa-check"></i><b>2.2</b> Regression Models and Their Uses</a></li>
<li class="chapter" data-level="2.3" data-path="linear-regression-with-one-predictor-variable.html"><a href="linear-regression-with-one-predictor-variable.html#simple-linear-regression-model-with-distribution-of-error-terms-unspecified"><i class="fa fa-check"></i><b>2.3</b> Simple linear regression model with distribution of error terms unspecified</a></li>
<li class="chapter" data-level="2.4" data-path="linear-regression-with-one-predictor-variable.html"><a href="linear-regression-with-one-predictor-variable.html#data-for-regression-analysis"><i class="fa fa-check"></i><b>2.4</b> Data for regression analysis</a></li>
<li class="chapter" data-level="2.5" data-path="linear-regression-with-one-predictor-variable.html"><a href="linear-regression-with-one-predictor-variable.html#overview-of-steps-in-regression-analysis"><i class="fa fa-check"></i><b>2.5</b> Overview of steps in regression analysis</a></li>
<li class="chapter" data-level="2.6" data-path="linear-regression-with-one-predictor-variable.html"><a href="linear-regression-with-one-predictor-variable.html#estimation-of-regression-function"><i class="fa fa-check"></i><b>2.6</b> Estimation of regression function</a></li>
<li class="chapter" data-level="2.7" data-path="linear-regression-with-one-predictor-variable.html"><a href="linear-regression-with-one-predictor-variable.html#estimation-of-erro-terms-variance-sigma2"><i class="fa fa-check"></i><b>2.7</b> Estimation of Erro Terms Variance <span class="math inline">\(\sigma^{2}\)</span></a></li>
<li class="chapter" data-level="2.8" data-path="linear-regression-with-one-predictor-variable.html"><a href="linear-regression-with-one-predictor-variable.html#normal-error-regression-model"><i class="fa fa-check"></i><b>2.8</b> Normal Error Regression Model</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="inferences-in-regeression-and-correlation-analysis.html"><a href="inferences-in-regeression-and-correlation-analysis.html"><i class="fa fa-check"></i><b>3</b> Inferences in Regeression and Correlation Analysis</a><ul>
<li class="chapter" data-level="3.1" data-path="inferences-in-regeression-and-correlation-analysis.html"><a href="inferences-in-regeression-and-correlation-analysis.html#inferences-concerning-beta_1"><i class="fa fa-check"></i><b>3.1</b> Inferences Concerning <span class="math inline">\(\beta_{1}\)</span></a></li>
<li class="chapter" data-level="3.2" data-path="inferences-in-regeression-and-correlation-analysis.html"><a href="inferences-in-regeression-and-correlation-analysis.html#inferences-concerning-beta_0"><i class="fa fa-check"></i><b>3.2</b> Inferences Concerning <span class="math inline">\(\beta_{0}\)</span></a></li>
<li class="chapter" data-level="3.3" data-path="inferences-in-regeression-and-correlation-analysis.html"><a href="inferences-in-regeression-and-correlation-analysis.html#some-considerations-on-making-inferences-concerning-beta_0-and-beta_1"><i class="fa fa-check"></i><b>3.3</b> Some Considerations on Making Inferences Concerning <span class="math inline">\(\beta_{0}\)</span> and <span class="math inline">\(\beta_{1}\)</span></a></li>
<li class="chapter" data-level="3.4" data-path="inferences-in-regeression-and-correlation-analysis.html"><a href="inferences-in-regeression-and-correlation-analysis.html#interval-estimation-of-ey_h"><i class="fa fa-check"></i><b>3.4</b> Interval Estimation of <span class="math inline">\(E(Y_{h})\)</span></a></li>
<li class="chapter" data-level="3.5" data-path="inferences-in-regeression-and-correlation-analysis.html"><a href="inferences-in-regeression-and-correlation-analysis.html#prediction-of-new-observation"><i class="fa fa-check"></i><b>3.5</b> Prediction of New Observation</a></li>
<li class="chapter" data-level="3.6" data-path="inferences-in-regeression-and-correlation-analysis.html"><a href="inferences-in-regeression-and-correlation-analysis.html#confidence-band-for-regression-line"><i class="fa fa-check"></i><b>3.6</b> Confidence Band for Regression Line</a></li>
<li class="chapter" data-level="3.7" data-path="inferences-in-regeression-and-correlation-analysis.html"><a href="inferences-in-regeression-and-correlation-analysis.html#analysis-of-variance-approach"><i class="fa fa-check"></i><b>3.7</b> Analysis of Variance Approach</a></li>
<li class="chapter" data-level="3.8" data-path="inferences-in-regeression-and-correlation-analysis.html"><a href="inferences-in-regeression-and-correlation-analysis.html#general-linear-test-approach"><i class="fa fa-check"></i><b>3.8</b> General Linear Test Approach</a></li>
<li class="chapter" data-level="3.9" data-path="inferences-in-regeression-and-correlation-analysis.html"><a href="inferences-in-regeression-and-correlation-analysis.html#descriptive-measures-of-linear-association-between-x-and-y"><i class="fa fa-check"></i><b>3.9</b> Descriptive Measures of Linear Association between X and Y</a></li>
<li class="chapter" data-level="3.10" data-path="inferences-in-regeression-and-correlation-analysis.html"><a href="inferences-in-regeression-and-correlation-analysis.html#considerations-in-applying-regression-analysis"><i class="fa fa-check"></i><b>3.10</b> Considerations in Applying Regression Analysis</a></li>
<li class="chapter" data-level="3.11" data-path="inferences-in-regeression-and-correlation-analysis.html"><a href="inferences-in-regeression-and-correlation-analysis.html#normal-correlation-models"><i class="fa fa-check"></i><b>3.11</b> Normal Correlation Models</a></li>
<li class="chapter" data-level="3.12" data-path="inferences-in-regeression-and-correlation-analysis.html"><a href="inferences-in-regeression-and-correlation-analysis.html#r-code"><i class="fa fa-check"></i><b>3.12</b> R code</a><ul>
<li class="chapter" data-level="3.12.1" data-path="inferences-in-regeression-and-correlation-analysis.html"><a href="inferences-in-regeression-and-correlation-analysis.html#example-data"><i class="fa fa-check"></i><b>3.12.1</b> Example data</a></li>
<li class="chapter" data-level="3.12.2" data-path="inferences-in-regeression-and-correlation-analysis.html"><a href="inferences-in-regeression-and-correlation-analysis.html#built-in-function"><i class="fa fa-check"></i><b>3.12.2</b> built-in function</a></li>
<li class="chapter" data-level="3.12.3" data-path="inferences-in-regeression-and-correlation-analysis.html"><a href="inferences-in-regeression-and-correlation-analysis.html#point-estimator-b_0-and-b_1"><i class="fa fa-check"></i><b>3.12.3</b> point estimator <span class="math inline">\(b_0\)</span> and <span class="math inline">\(b_1\)</span></a></li>
<li class="chapter" data-level="3.12.4" data-path="inferences-in-regeression-and-correlation-analysis.html"><a href="inferences-in-regeression-and-correlation-analysis.html#residuals-sse-and-mse"><i class="fa fa-check"></i><b>3.12.4</b> Residuals, SSE and MSE</a></li>
<li class="chapter" data-level="3.12.5" data-path="inferences-in-regeression-and-correlation-analysis.html"><a href="inferences-in-regeression-and-correlation-analysis.html#sampling-distribution-of-b_1-and-b_1beta_1sb_1"><i class="fa fa-check"></i><b>3.12.5</b> sampling distribution of <span class="math inline">\(b_1\)</span> and <span class="math inline">\((b_1−\beta_1)/s(b_1)\)</span></a></li>
<li class="chapter" data-level="3.12.6" data-path="inferences-in-regeression-and-correlation-analysis.html"><a href="inferences-in-regeression-and-correlation-analysis.html#f-test"><i class="fa fa-check"></i><b>3.12.6</b> F test</a></li>
<li class="chapter" data-level="3.12.7" data-path="inferences-in-regeression-and-correlation-analysis.html"><a href="inferences-in-regeression-and-correlation-analysis.html#r2-and-r"><i class="fa fa-check"></i><b>3.12.7</b> <span class="math inline">\(R^2\)</span> and r</a></li>
<li class="chapter" data-level="3.12.8" data-path="inferences-in-regeression-and-correlation-analysis.html"><a href="inferences-in-regeression-and-correlation-analysis.html#plot"><i class="fa fa-check"></i><b>3.12.8</b> plot</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="4" data-path="diagnostics-and-remedial-measures.html"><a href="diagnostics-and-remedial-measures.html"><i class="fa fa-check"></i><b>4</b> Diagnostics and Remedial Measures</a></li>
<li class="chapter" data-level="5" data-path="simultaneous-inferences-and-other-topics-in-regression-analysis.html"><a href="simultaneous-inferences-and-other-topics-in-regression-analysis.html"><i class="fa fa-check"></i><b>5</b> Simultaneous Inferences and Other Topics in Regression Analysis</a></li>
<li class="chapter" data-level="6" data-path="matrix-approach-to-simple-linear-regression-analysis.html"><a href="matrix-approach-to-simple-linear-regression-analysis.html"><i class="fa fa-check"></i><b>6</b> Matrix Approach to Simple Linear Regression Analysis</a><ul>
<li class="chapter" data-level="6.1" data-path="matrix-approach-to-simple-linear-regression-analysis.html"><a href="matrix-approach-to-simple-linear-regression-analysis.html#matrices"><i class="fa fa-check"></i><b>6.1</b> Matrices</a></li>
<li class="chapter" data-level="6.2" data-path="matrix-approach-to-simple-linear-regression-analysis.html"><a href="matrix-approach-to-simple-linear-regression-analysis.html#matrix-addition-and-subtraction"><i class="fa fa-check"></i><b>6.2</b> Matrix Addition and Subtraction</a></li>
<li class="chapter" data-level="6.3" data-path="matrix-approach-to-simple-linear-regression-analysis.html"><a href="matrix-approach-to-simple-linear-regression-analysis.html#matrix-multiplication"><i class="fa fa-check"></i><b>6.3</b> Matrix Multiplication</a></li>
<li class="chapter" data-level="6.4" data-path="matrix-approach-to-simple-linear-regression-analysis.html"><a href="matrix-approach-to-simple-linear-regression-analysis.html#special-types-of-matrices"><i class="fa fa-check"></i><b>6.4</b> Special Types of Matrices</a></li>
<li class="chapter" data-level="6.5" data-path="matrix-approach-to-simple-linear-regression-analysis.html"><a href="matrix-approach-to-simple-linear-regression-analysis.html#linear-dependence-and-rank-of-matrix"><i class="fa fa-check"></i><b>6.5</b> Linear Dependence and Rank of Matrix</a></li>
<li class="chapter" data-level="6.6" data-path="matrix-approach-to-simple-linear-regression-analysis.html"><a href="matrix-approach-to-simple-linear-regression-analysis.html#inverse-of-a-matrix"><i class="fa fa-check"></i><b>6.6</b> Inverse of a Matrix</a></li>
<li class="chapter" data-level="6.7" data-path="matrix-approach-to-simple-linear-regression-analysis.html"><a href="matrix-approach-to-simple-linear-regression-analysis.html#some-basic-results-for-matrics"><i class="fa fa-check"></i><b>6.7</b> Some Basic Results for Matrics</a></li>
<li class="chapter" data-level="6.8" data-path="matrix-approach-to-simple-linear-regression-analysis.html"><a href="matrix-approach-to-simple-linear-regression-analysis.html#random-vectors-and-matrices"><i class="fa fa-check"></i><b>6.8</b> Random Vectors and Matrices</a></li>
<li class="chapter" data-level="6.9" data-path="matrix-approach-to-simple-linear-regression-analysis.html"><a href="matrix-approach-to-simple-linear-regression-analysis.html#simple-linear-regression-model-in-matrix-terms"><i class="fa fa-check"></i><b>6.9</b> Simple Linear Regression Model in Matrix Terms</a></li>
<li class="chapter" data-level="6.10" data-path="matrix-approach-to-simple-linear-regression-analysis.html"><a href="matrix-approach-to-simple-linear-regression-analysis.html#leasst-squares-estimation-of-regression-parameters"><i class="fa fa-check"></i><b>6.10</b> Leasst Squares Estimation of Regression Parameters</a></li>
<li class="chapter" data-level="6.11" data-path="matrix-approach-to-simple-linear-regression-analysis.html"><a href="matrix-approach-to-simple-linear-regression-analysis.html#fitted-values-and-residuals"><i class="fa fa-check"></i><b>6.11</b> Fitted Values and Residuals</a></li>
<li class="chapter" data-level="6.12" data-path="matrix-approach-to-simple-linear-regression-analysis.html"><a href="matrix-approach-to-simple-linear-regression-analysis.html#analysis-of-variance-results"><i class="fa fa-check"></i><b>6.12</b> Analysis of Variance Results</a></li>
<li class="chapter" data-level="6.13" data-path="matrix-approach-to-simple-linear-regression-analysis.html"><a href="matrix-approach-to-simple-linear-regression-analysis.html#inferences-in-regeression-analysis"><i class="fa fa-check"></i><b>6.13</b> Inferences in Regeression Analysis</a></li>
<li class="chapter" data-level="6.14" data-path="matrix-approach-to-simple-linear-regression-analysis.html"><a href="matrix-approach-to-simple-linear-regression-analysis.html#r-code-1"><i class="fa fa-check"></i><b>6.14</b> R code</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="multiple-linear-regression-i.html"><a href="multiple-linear-regression-i.html"><i class="fa fa-check"></i><b>7</b> Multiple linear regression I</a><ul>
<li class="chapter" data-level="7.1" data-path="multiple-linear-regression-i.html"><a href="multiple-linear-regression-i.html#multiple-regression-models"><i class="fa fa-check"></i><b>7.1</b> Multiple Regression Models</a><ul>
<li class="chapter" data-level="7.1.1" data-path="multiple-linear-regression-i.html"><a href="multiple-linear-regression-i.html#first-order-model-with-two-predictor-variables"><i class="fa fa-check"></i><b>7.1.1</b> First-Order Model with Two Predictor Variables</a></li>
<li class="chapter" data-level="7.1.2" data-path="multiple-linear-regression-i.html"><a href="multiple-linear-regression-i.html#first-order-model-with-more-than-two-predictor-variables"><i class="fa fa-check"></i><b>7.1.2</b> First-Order Model with More than Two Predictor Variables</a></li>
<li class="chapter" data-level="7.1.3" data-path="multiple-linear-regression-i.html"><a href="multiple-linear-regression-i.html#general-linear-regression-model"><i class="fa fa-check"></i><b>7.1.3</b> General Linear Regression Model</a></li>
</ul></li>
<li class="chapter" data-level="7.2" data-path="multiple-linear-regression-i.html"><a href="multiple-linear-regression-i.html#general-linear-regression-model-in-matrix-terms"><i class="fa fa-check"></i><b>7.2</b> General Linear Regression Model in Matrix Terms</a></li>
<li class="chapter" data-level="7.3" data-path="multiple-linear-regression-i.html"><a href="multiple-linear-regression-i.html#estimation-of-regression-coefficients"><i class="fa fa-check"></i><b>7.3</b> Estimation of Regression Coefficients</a></li>
<li class="chapter" data-level="7.4" data-path="multiple-linear-regression-i.html"><a href="multiple-linear-regression-i.html#fitted-values-and-residuals-1"><i class="fa fa-check"></i><b>7.4</b> Fitted Values and Residuals</a></li>
<li class="chapter" data-level="7.5" data-path="multiple-linear-regression-i.html"><a href="multiple-linear-regression-i.html#analysis-of-variance-results-1"><i class="fa fa-check"></i><b>7.5</b> Analysis of Variance Results</a></li>
<li class="chapter" data-level="7.6" data-path="multiple-linear-regression-i.html"><a href="multiple-linear-regression-i.html#inferences-about-regression-parameters"><i class="fa fa-check"></i><b>7.6</b> Inferences about Regression Parameters</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="multiple-regression-ii.html"><a href="multiple-regression-ii.html"><i class="fa fa-check"></i><b>8</b> Multiple Regression II</a><ul>
<li class="chapter" data-level="8.1" data-path="multiple-regression-ii.html"><a href="multiple-regression-ii.html#extra-sums-of-squares"><i class="fa fa-check"></i><b>8.1</b> Extra Sums of Squares</a></li>
</ul></li>
<li class="divider"></li>
Published with <a href="https://github.com/rstudio/bookdown" target="blank">bookdown</a> by <a href="http://zjuwhw.github.io/" target="blank">zjuwhw</a>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./"></a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="linear-regression-with-one-predictor-variable" class="section level1">
<h1><span class="header-section-number">2</span> Linear regression with one predictor variable</h1>
<div id="relations-between-variables" class="section level2">
<h2><span class="header-section-number">2.1</span> Relations between variables</h2>
<ul>
<li>relation
<ul>
<li><em>Function relation</em>: Y = f(X), e.g. total cost = the number of products * cost per product</li>
<li><em>Statistical relation</em>: not a perfect one, e.g. performance for 10 employees at midyear and year-end</li>
</ul></li>
<li>variable
<ul>
<li>X: <em>independent/explanatory/predictor variable</em></li>
<li>Y: <em>dependent/response variable</em></li>
</ul></li>
<li>plot
<ul>
<li><em>scatter diagram/plot</em></li>
<li>each point represents a <em>trial</em> or a <em>case</em></li>
</ul></li>
</ul>
</div>
<div id="regression-models-and-their-uses" class="section level2">
<h2><span class="header-section-number">2.2</span> Regression Models and Their Uses</h2>
<ul>
<li>History
<ul>
<li><a href="https://en.wikipedia.org/wiki/Francis_Galton">Sir Francis Galton</a> in the latter part of 19th century</li>
<li>relation between heights of parents and children</li>
<li>regression to the mean</li>
</ul></li>
<li>Basic Concepts
<ul>
<li>A regression model</li>
<li>two characters:
<ul>
<li>there is a probability distribution of Y for each level of X</li>
<li>The means of these probability distribution vary in some systematic fashion with X</li>
</ul></li>
<li><em>regression function</em>: the systematic relationship
<ul>
<li><em>linear</em>, <em>curvilinear</em>, etc.</li>
</ul></li>
<li><em>regression curve</em>: the graph of the regression function</li>
<li>probability distributions: <em>symmetrical</em>, <em>skewed</em> etc.</li>
</ul></li>
<li>Regression models with more than one predictor variable</li>
<li>Construction of Regression Models
<ul>
<li>Selection of predictor variables</li>
<li>Functional form of regression relation</li>
<li>Scope of model</li>
</ul></li>
<li>Uses of regression analysis
<ul>
<li>description</li>
<li>control</li>
<li>prediction</li>
<li>overlap in practice</li>
</ul></li>
<li>Regeression and causality</li>
<li>Use of Computers</li>
</ul>
</div>
<div id="simple-linear-regression-model-with-distribution-of-error-terms-unspecified" class="section level2">
<h2><span class="header-section-number">2.3</span> Simple linear regression model with distribution of error terms unspecified</h2>
<ul>
<li>Formal statement of model</li>
</ul>
<p><span class="math display">\[Y_{i} = \beta_{0} + \beta_{1}X_{i} + \varepsilon_{i}\]</span></p>
<p>where:</p>
<ul>
<li><span class="math inline">\(Y_{i}\)</span> is the value of th response variable in the ith trail</li>
<li><span class="math inline">\(\beta_{0}\text{ and }\beta_{1}\)</span> are paramters</li>
<li><span class="math inline">\(X_{i}\)</span> is a known constant, namely, the value of the predictor variable in the ith trial</li>
<li><span class="math inline">\(\varepsilon_{i}\)</span> is a random error term
<ul>
<li>mean <span class="math inline">\(E(\varepsilon_{i}) = 0\)</span></li>
<li>variance <span class="math inline">\(\sigma^{2}(\varepsilon_{i}) = \sigma^{2}\)</span></li>
<li>covariance <span class="math inline">\(\sigma(\varepsilon_{i}, \varepsilon_{j}) = 0\)</span>, for all i, j; <span class="math inline">\(i \neq j\)</span></li>
</ul></li>
<li>Important features of model</li>
</ul>
<ol style="list-style-type: decimal">
<li><span class="math inline">\(Y_{i}\)</span> contains two components: the constant term <span class="math inline">\(\beta_{0} + \beta_{1}X_{i}\)</span> and the random term <span class="math inline">\(\varepsilon_{i}\)</span>. Hence, <span class="math inline">\(Y_{i}\)</span> is a random variable</li>
<li>Since <span class="math inline">\(E(\varepsilon_{i}) = 0\)</span>, <span class="math inline">\(E(Y_{i}) = E(\beta_{0} + \beta_{1}X_{i} + \varepsilon_{i}) = \beta_{0} + \beta_{1}X_{i} + E(\varepsilon_{i}) = \beta_{0} + \beta_{1}X_{i}\)</span></li>
<li>The response <span class="math inline">\(Y_{i}\)</span> in the ith trail exceeds or falls short of the value of the regssion fucntion by the error term amount <span class="math inline">\(\varepsilon_{i}\)</span></li>
<li>The erorr term <span class="math inline">\(\varepsilon_{i}\)</span> are assumed to have constant variance <span class="math inline">\(\sigma^{2}\)</span> and <span class="math inline">\(\sigma^{2}(Y_{i}) = \sigma^{2}\)</span></li>
<li>The error terms are assumed to be uncorrelated, so are the responses <span class="math inline">\(Y_{i}\)</span> and <span class="math inline">\(Y_{j}\)</span></li>
</ol>
<ul>
<li>Meaning of regression paramters
<ul>
<li><em>regrssion coefficients</em>: the paramters <span class="math inline">\(\beta_{0}\text{ and }\beta_{1}\)</span></li>
<li><em>the slope of the regression line</em>: <span class="math inline">\(\beta_{1}\)</span></li>
</ul></li>
<li>Alternative versions of regression model</li>
</ul>
<p><span class="math display">\[Y_{i} = \beta_{0}X_{0} + \beta_{1}X_{i} + \varepsilon_{i}\text{, where }X_{0} \equiv 1\]</span></p>
<p><span class="math display">\[Y_{i} = \beta_{0}^{*} + \beta_{1}(X_{i} - \bar{X}) + \varepsilon_{i}\text{, where }\beta_{0}^{*} = \beta_{0} + \beta_{1}\bar{X}\]</span></p>
</div>
<div id="data-for-regression-analysis" class="section level2">
<h2><span class="header-section-number">2.4</span> Data for regression analysis</h2>
<ul>
<li>Observational Data</li>
<li>Eperimental Data
<ul>
<li>treatment</li>
<li>experimental units</li>
</ul></li>
<li>Completely randomized design</li>
</ul>
</div>
<div id="overview-of-steps-in-regression-analysis" class="section level2">
<h2><span class="header-section-number">2.5</span> Overview of steps in regression analysis</h2>
</div>
<div id="estimation-of-regression-function" class="section level2">
<h2><span class="header-section-number">2.6</span> Estimation of regression function</h2>
<ul>
<li>Methods of Least Squares
<ul>
<li>To find estimates <span class="math inline">\(b_{0}\)</span> and <span class="math inline">\(b_{1}\)</span> for <span class="math inline">\(\beta_{0}\)</span> and <span class="math inline">\(\beta_{1}\)</span>, respectively, for which Q is a minimum, where <span class="math inline">\(Q = \displaystyle\sum_{i=1}^{n}(Y_{i} - \beta_{0} - \beta_{1}X_{i})^2\)</span>.</li>
<li>Least Squares Estimators <span class="math inline">\(b_{0}\)</span> and <span class="math inline">\(b_{1}\)</span> can be found in two ways:
<ul>
<li>numerical search procedures</li>
<li>analytical procedures</li>
</ul></li>
</ul></li>
</ul>
<p><span class="math display">\[b_{1} = \frac{\sum(X_{i} - \bar{X})(Y_{i} - \bar{Y})}{\sum(X_{i} - \bar{X})^2}\]</span></p>
<p><span class="math display">\[b_{0} = \frac{1}{n}(\sum Y_{i} - b_{1} \sum X_{i}) = \bar{Y} - b_{1}\bar{X}\]</span></p>
<ul>
<li>Proof</li>
</ul>
<p>The paritial derivatives are</p>
<p><span class="math display">\[\frac{\partial Q}{\partial\beta_{0}} = -2\sum(Y_{i} - \beta_{0} - \beta_{1}X_{i})\]</span></p>
<p><span class="math display">\[\frac{\partial Q}{\partial\beta_{1}} = -2\sum X_{i}(Y_{i} - \beta_{0} - \beta_{1}X_{i})\]</span></p>
<p>We set them equal to zero, using <span class="math inline">\(b_{0}\)</span> and <span class="math inline">\(b_{1}\)</span> to denote the particular values of <span class="math inline">\(b_{0}\)</span> and <span class="math inline">\(b_{1}\)</span> that minimize Q:</p>
<p><span class="math display">\[-2\sum(Y_{i} - \beta_{0} - \beta_{1}X_{i}) = 0\]</span></p>
<p><span class="math display">\[-2\sum X_{i}(Y_{i} - \beta_{0} - \beta_{1}X_{i}) = 0\]</span></p>
<ul>
<li>Proerties of Least Squares Estimators</li>
</ul>
<p>Guass-Markov theorem:</p>
<blockquote>
<p>Under the conditions of regression model, the least squares estimators b0 and b1 are unbiased and have minimum variance among all unbiased linear estimators</p>
</blockquote>
<ul>
<li>Point Esitmation of Mean Response
<ul>
<li>estimate the regression function as follows:</li>
</ul>
<p><span class="math display">\[\hat{Y} = b_{0} + b_{1}X\]</span></p></li>
<li>Residuals
<ul>
<li>residual: the differenc between the observed value <span class="math inline">\(Y_{i}\)</span> and the corresponding fitted value <span class="math inline">\(\hat{Y_{i}}\)</span></li>
</ul></li>
</ul>
<p><span class="math display">\[e_{i} = Y_{i} - \hat{Y}_{i}\]</span></p>
<ul>
<li>Properties of Fitted Regression Line
<ul>
<li><span class="math inline">\(\sum e_{i} = 0\)</span></li>
<li><span class="math inline">\(\sum e_{i}^{2}\)</span> is a minimum</li>
<li><span class="math inline">\(\sum Y_{i} = \sum \hat{Y}_{i}\)</span></li>
<li><span class="math inline">\(\sum X_{i} e_{i} = 0\)</span></li>
<li><span class="math inline">\(\sum \hat{Y}_{i}\)</span>e<sub>i</sub> = 0</li>
<li>the regression line always goes through the point <span class="math inline">\((\bar{X}, \bar{Y})\)</span></li>
</ul></li>
</ul>
</div>
<div id="estimation-of-erro-terms-variance-sigma2" class="section level2">
<h2><span class="header-section-number">2.7</span> Estimation of Erro Terms Variance <span class="math inline">\(\sigma^{2}\)</span></h2>
<ul>
<li>Point Estimator of <span class="math inline">\(\sigma^{2}\)</span>
<ul>
<li>Single population
<ul>
<li>sum of squares: <span class="math inline">\(\displaystyle\sum_{i=1}^{n}(Y_{i}-\bar{Y})^2\)</span></li>
<li>degrees of freedom (df): n - 1, because one degree of freedom is lost by using <span class="math inline">\(\bar{Y}\)</span> as an estimate of the unknown population mean <span class="math inline">\(\mu\)</span></li>
<li>sample variance/mean square: <span class="math inline">\(s^2 = \frac{\displaystyle\sum(Y_{i}-\bar{Y})^2}{n-1}\)</span></li>
</ul></li>
<li>Regression model
<ul>
<li>deviation/residual: <span class="math inline">\(Y_{i} - \hat{Y}_{i}\)</span> = e<sub>i</sub></li>
<li>error/residual sum of squares:
<ul>
<li><span class="math inline">\(SSE = \displaystyle\sum_{i=1}^{n}(Y_{i} - \hat{Y}_{i})^{2}\)</span></li>
<li><span class="math inline">\(SSE = \displaystyle\sum_{i=1}^{n}e_{i}^{2}\)</span></li>
</ul></li>
<li>degrees of freedom: n - 2, because two degrees of freedom are lost due to estimating <span class="math inline">\(\beta_{0}\)</span> and <span class="math inline">\(\beta_{1}\)</span> to get <span class="math inline">\(\hat{Y}_{i}\)</span></li>
<li>MSE (error/residual mean square): <span class="math inline">\(s^{2} = MSE = \frac{SSE}{n-2}\)</span></li>
</ul></li>
</ul></li>
</ul>
</div>
<div id="normal-error-regression-model" class="section level2">
<h2><span class="header-section-number">2.8</span> Normal Error Regression Model</h2>
<ul>
<li>Model
<ul>
<li>same with simple linear regression model</li>
<li>except it assumes that the error <span class="math inline">\(\varepsilon_{i}\)</span> are normally distributed</li>
</ul></li>
<li>Estimation of Parameters by Method of Maximum Likelihood</li>
</ul>
<p>Method of maximum likelihood chooses as estimates those values of the parameters that are most consistent with the sample data.</p>
<pre><code>A normal distribution with SD = 10, mean is unknown
A random of sample n = 3 yields the results 250, 265 and 259
The likelihood value (L) is the product of the densities of the normal distribution

If we assue μ = 230, L(μ = 230) = 0.279*10E-9 
R code: prod(dnorm(c(250,265,259), 230, 10))
If we assue μ = 259, L(μ = 259) = 0.0000354
R code: prod(dnorm(c(250,265,259), 259, 10))

So, L(μ = 259) &gt; L(μ = 230)

The method of maximum likelihood is to estimate prarametes to get maximum L.
It can be shown that for a normal population,
the maximum likelihood estimator of μ is the smaple mean</code></pre>
<p>Fro regression model, the likelihood function for n observations is</p>
<p><span class="math display">\[L(\beta_{0}, \beta_{1}, \sigma^{2}) = \displaystyle\Pi_{i=1}^{n}\frac{1}{(2\pi\sigma^{2})^{1/2}}exp[-\frac{1}{2\sigma^{2}}(Y_{i} - \beta_{0} - \beta_{1}X_{i})^{2}]\]</span></p>
<p><span class="math display">\[L(\beta_{0}, \beta_{1}, \sigma^{2}) = \frac{1}{(2\pi\sigma^{2})^{1/2}}exp[-\frac{1}{2\sigma^{2}}\displaystyle\sum_{i=1}^{n}(Y_{i} - \beta_{0} - \beta_{1}X_{i})^{2}]\]</span></p>

</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="index.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="inferences-in-regeression-and-correlation-analysis.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"google": false,
"weibo": false,
"instapper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"download": null,
"toc": {
"collapse": "none"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://cdn.bootcss.com/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:" && /^https?:/.test(script.src))
      script.src  = script.src.replace(/^https?:/, '');
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
