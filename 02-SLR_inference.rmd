# Inferences in Regeression and Correlation Analysis

## Inferences Concerning $\beta_{1}$

- Sampling Distribution of b1

The sampling distribution of b1 refers to the different values of b1 that would be obtained with repeated sampling when the levels of the predictor variable X are held constant from sample to sample.

For normal error regression model, the sample distributon of b1 is **normal**, with mean and variance:

$$E(b_1) = \beta_{1}$$

$$\sigma^{2}(b_1) = \frac{\sigma^{2}}{\sum(X_{i} - \bar{X})^{2}}$$

- Proof

b1 as linear combination of the Yi

$$b1 = \sum k_{i}Y_{i}\text{ where }k_{i} = \frac{X_{i} - \bar{X}}{\sum(X_{i} - \bar{X})^{2}}$$
- Nomaily

The $Y_{i}$ are independently, normally distributed, so b1 are normally distributed.

- Mean

$$E(b_{1}) = E(\sum k_{i}Y_{i}) = \sum k_{i}E(Y_{i}) = \sum k_{i}(\beta_{0} + \beta_{1}X_{i}) = \beta_{1}$$

hint:

$$\sum k_{i} = 0$$

$$\sum k_{i}X_{i} = 1$$

- Variance

$$\sigma^{2}(b_{1}) = \sigma^{2}(\sum k_{i}Y_{i}) = \sum k_{i}^{2}\sigma^{2}(Y_{i}) = \sum k_{i}^{2}\sigma^{2} = \sigma^{2}\frac{1}{\sum (X_{i} - \bar{X})^{2}}$$

- Estimated Variance

Replace the paramter $\sigma^{2}$ with MSE:

$$s^{2}(b_{1}) = \frac{MSE}{\sum(X_{i} - \bar{X})^{2}}$$


- Sampling Distribution of $(b_{1} - \beta_{1})/s(b_{1})$

$$(b_{1} - \beta_{1})/\sigma(b_{1}) \sim N(0,1)$$

$$(b_{1} - \beta_{1})/s(b_{1}) \sim t(n-2)$$

When a statistic is standardized but the denominator is an estimated standard deviation rather than the true standard deviation, it is called a studentized statistic.

- Comment

$$SSE/\sigma^{2} \sim \chi^{2}(n - 2)$$

$$(b_{1} - \beta_{1})/s(b_{1}) \sim \frac{z}{\sqrt{\frac{\chi^2(n-2)}{n-2}}} = t(n-2)$$

- Confidence Interval for $\beta_{1}$

$$b_{1} \pm t(1-\alpha/2; n-2)s(b_{1})\text{ where }\alpha\text{ is significance level}$$

- Tests concerning $\beta_{1}$

Since $(b_{1} - \beta_{1})/s(b_{1})$ is ditributed as t with n - 2degrees of freedom, tests concerning $\beta_{1}$ can be set up in ordinary fashion using the t distribution.


## Inferences Concerning $\beta_{0}$

The sampling distribution of $\beta_{0}$ is normal, with mean and variance:

$$E(b_{0}) = \beta_{0}$$

$$\sigma^{2}(b_{0}) = \sigma^{2}[\frac{1}{n} + \frac{\bar{X}^{2}}{\sum (X_{i} - \bar{X})^{2}}]$$

$$s^{2}(b_{0}) = MSE[\frac{1}{n} + \frac{\bar{X}^{2}}{\sum (X_{i} - \bar{X})^{2}}]$$

$$\frac{b_{0} - \beta_{0}}{s(b_{0})} \sim t(n-2)$$

## Some Considerations on Making Inferences Concerning $\beta_{0}$ and $\beta_{1}$

- Effects of Departures from Normality
- Interpretation of Confidence Coefficient and Risks of Errors
- Spacing of the X levels
- Power of Tests

The power of this test is the probability that the decision rule will lead to conclusion $H_{a}$ when $H_{a}$ in fact holds. Specifically, the power is given by

$$Power = P(|t^{*}| > t(1-\alpha/2;n-2)|\delta)$$

where,

- $H_{0}: \beta_{1} = \beta_{10}$; $H_{a}: \beta_{1} \neq \beta_{10}$
- $t^{*} = \frac{b_{1} - \beta_{10}}{s(b_{1})}$
- $\delta$ is the **noncentrality measure**, a measure of how far the true value of $\beta_{1}$ is from $\beta_{10}$. $\delta = \frac{\mid\beta_{1} - \beta_{10}\mid}{\sigma(b_{1})}$

## Interval Estimation of $E(Y_{h})$

The mean response when $X = X_{h}$ is denoted by $E(Y_{h})$. The $E(Y_{h})$ point estimator $\hat{Y}_{h}$ :

$$\hat{Y}_{h} = b_{0} + b_{1}X_{h}$$

- Sampling Distribution of $\hat{Y}_{h}$

For normal error regression model, the sampling distribution of $\hat{Y}_{h}$ is normal, with mean and variance:

$$E(\hat{Y}_{h}) = E(Y_{h})$$

$$\sigma^{2}(\hat{Y}_{h}) = \sigma^{2}[\frac{1}{n} + \frac{(X_{h} - \bar{X})^2}{\sum(X_{i} - \bar{X})^{2}}]$$

$$s^{2}(\hat{Y}_{h}) = MSE[\frac{1}{n} + \frac{(X_{h} - \bar{X})^{2}}{\sum (X_{i} - \bar{X})^{2}}]$$

$$\frac{\hat{Y}_{h} - E(Y_{h})}{s(\hat{Y}_{h})} \sim t(n-2)$$

## Prediction of New Observation

We denote the level of X for the new trial as $X_{h}$ and the new observation on Y as $Y_{h(new)}$.

In the former case, the estimation of $E(Y_{h})$ is the **mean** of the distribution of Y; in the present case, we predict an **individual outcome** draw from the distribution of Y.

Hence, two components of $\sigma(pred)$:

1. The variance of the distribution of Y at $X = X_{h}$, namely $\sigma^{2}$
2. The variance of the sampling distribution of $\hat{Y}_h$, namely $\sigma^{2}(\hat{Y}_h)$

$$\sigma^{2}(pred) = \sigma^{2}(Y_{h(new)} - \hat{Y}_{h}) = \sigma^{2} + \sigma^{2}(\hat{Y}_{h})$$

$$s^{2}(pred) = MSE[1 + \frac{1}{n} + \frac{(X_{h} - \bar{X})^{2}}{\sum (X_{i} - \bar{X})^{2}}]$$

## Confidence Band for Regression Line

To obtain a confidence band for the entire for the entire regression line $E(Y) = \beta_{0} + \beta_{1}X$.

The **Working-Hotellling** 1 - $\alpha$ confidence band:

$$\hat{Y}_{h} \pm Ws(\hat{Y}_{h})$$

where,

$$W^{2} = 2F(1-\alpha; 2, n-2)$$

Since, we are doing all values of $X_{h}$ at once, it will be wider at each $X_{h}$ than CIs for individual $X_{h}$.

## Analysis of Variance Approach

- Partitioning of Total Sum of Squares

$$Y_{i} - \bar{Y} = \hat{Y}_{i} - \bar{Y} + Y_{i} - \hat{Y}_{i}$$

$$\sum (Y_{i} - \bar{Y})^{2} = \sum (\hat{Y}_{i} - \bar{Y})^{2} + \sum (Y_{i} - \hat{Y}_{i})^{2}$$

$$SSTO = SSR + SSE$$

SSTO stands for **total sum of squares**, SSE stands for **error sum of squares** and SSR stands for **regression sum of squares**.

- Breakdown of Degrees of Freedom

$$n - 1 = 1 + (n - 2)$$

We have n-1 degrees of freedom associated with SSTO. SSE has n-2 degrees of freedom and SSR has 1 degree of freedom.

- Mean Squares

A sum of squares divided by its associated degrees of freedom is called a **mean square** (MS)

The mean squares are not additive:


$\frac{SSTO}{n-1} \neq \frac{SSR}{1} + \frac{SSE}{n-2} = MSR + MSE$

- ANalysis Of VAriance Table (ANOVA table)

**ANOVA table**: The breakdowns of the total sum of squares and associated degrees of freedom are displayed in the form of ANVOA.

**SSTOU**: the total uncorrected sum of squares, $\sum Y_i^2$

**SS**: correction for the mean sum of squares, $n\bar{Y}^2$

**SSTO** = $\sum (Y_i - \bar{Y})^2 = \sum Y_i^2 - n\bar{Y}^2 = SSTOU + SS$

|Source of Variation|SS|df|MS|
|:--|:--|:--|:--|
|Regression|$SSR = \sum(\hat{Y}_i - \bar{Y})^2$|1|$MSR = \frac{SSR}{1}$|
|Error|$SSE = \sum(Y_i - \hat{Y}_i)^2$|n-2|$MSE = \frac{SSE}{n-2}$|
|Total|$SSTO = \sum(Y_i - \bar{Y})^2$|n-1|
|Correction for mean|$SS \text{(correction for mean)} = n\bar{Y}^2$|1|
|Total, uncorrected|$SSTOU = \sum Y_i^2$|n|

- Expected Mean Squares

$$E(MSE) = \sigma^2$$

$$E(MSR) = \sigma^2 + \beta_1^2 \sum (X_i - \bar{X})^2$$

- F test for $\beta_1 = 0$ versus $\beta_1 \neq 0$

Test Statistic: $F^* = \frac{MSR}{MSE} \sim F(1,n-2)$

## General Linear Test Approach

- Two models:
	- $Y_i = \beta_0 + \beta_2X_i + \varepsilon_i$ (full model)
	- $Y_i = \beta_0 + \varepsilon_i$ (reduced model under H0)
- F-statistic:

$$F = \frac{(SSE(R) - SSE(F))/(df_R - df_F)}{SSE(F)/df_F}$$

The general linear teest approach can be used for highly complex tests of linear statistical models, as well as for simple tests. The basic steps in summary form are:

1. Fit the full model and obtain the error sum of squares SSE(F)
2. Fit the reduced model under H0 and obtain the error sum of squares SSE(R)
3. Use the test statistic and desicison rule

## Descriptive Measures of Linear Association between X and Y

- Coefficient of Determination

$$R^2 = \frac{SSR}{SSTO} = 1 - \frac{SSE}{SSTO}, 0 \leq R^2 \leq 1$$

- Limitations of $R^2$

Tree common misunderstandings about $R^2$

1. A high coefficient of determination indicates that useful predictions can be made.
2. A high coefficient of determination indicates that the estimated regression line is a good fit.
3. A coefficient of determination near zero indicates that X and Y are not related.

- Coefficient of Correlation

$$r = \pm \sqrt{R^2}, -1 \leq r \leq 1$$

## Considerations in Applying Regression Analysis

1. make inferences
2. the predictor variable itsef often has to be predicted
3. the levels of the predictor variable that fall outside the range of observations
4. $\beta_1 \neq 0$ doesnot establish a cause-and-effect relation
5. multiple testing
6. observations on the predictor variable X are subject to measurement erros

## Normal Correlation Models

- Distinction between Regression and Correlation Model
- Bivariate Normal Distribution
- Conditional Inferences
- Inferences on Correlation Coefficients
- Spearman Rank Correlation Coefficient

## R code

### Example data

```{r}
head(trees)
X = trees$Volume ## 体积
Y = trees$Girth ## 直径
```

### built-in function

```{r, cache=TRUE}
fit <- lm(Y~X)
summary(fit)
coefficients(fit) # model coefficients
confint(fit, level=0.95) # CIs for model parameters 
fitted(fit) # predicted values
residuals(fit) # residuals
anova(fit) # anova table 
```

### point estimator $b_0$ and $b_1$

```{r, cache=TRUE}
n = nrow(trees)
Xbar = mean(X)
Ybar = mean(Y)
b1 = sum((X - Xbar)*(Y - Ybar))/sum((X-Xbar)^2)
b0 = Ybar - b1*Xbar
b1;b0
```

### Residuals, SSE and MSE

```{r,cache=TRUE}
residual = Y - b1*X - b0
SSE = sum(residual^2)
MSE = SSE/(n-2)
SSE; MSE; sqrt(MSE)
```

### sampling distribution of $b_1$ and $(b_1−\beta_1)/s(b_1)$

```{r, cache=TRUE}
s = sqrt( MSE/sum((X - Xbar)^2))
t = b1 / s
p = (1 - pt(t, n -2))*2
s; t; p
```

### F test

```{r,cache=TRUE}
SSTO = var(Y) * (n-1)
F = (SSTO - SSE)/((n-1) - (n-2)) / (SSE/(n-2))
F
```

### $R^2$ and r

```{r, cache=TRUE}
Rsq = 1 - SSE/SSTO
r = b1/abs(b1) * sqrt(Rsq) 
Rsq; r; cor(X,Y)
```

### plot

```{r, cache=TRUE}
plot(X,Y)
abline(b0,b1, col="red")
```