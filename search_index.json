[
["index.html", "1 Preface", " 1 Preface This is a notebook for reading the book “Applied Linear Regression Models (4th edition)”. "],
["linear-regression-with-one-predictor-variable.html", "2 Linear regression with one predictor variable 2.1 Relations between variables 2.2 Regression Models and Their Uses 2.3 Simple linear regression model with distribution of error terms unspecified 2.4 Data for regression analysis 2.5 Overview of steps in regression analysis 2.6 Estimation of regression function 2.7 Estimation of Erro Terms Variance \\(\\sigma^{2}\\) 2.8 Normal Error Regression Model", " 2 Linear regression with one predictor variable 2.1 Relations between variables relation Function relation: Y = f(X), e.g. total cost = the number of products * cost per product Statistical relation: not a perfect one, e.g. performance for 10 employees at midyear and year-end variable X: independent/explanatory/predictor variable Y: dependent/response variable plot scatter diagram/plot each point represents a trial or a case 2.2 Regression Models and Their Uses History Sir Francis Galton in the latter part of 19th century relation between heights of parents and children regression to the mean Basic Concepts A regression model two characters: there is a probability distribution of Y for each level of X The means of these probability distribution vary in some systematic fashion with X regression function: the systematic relationship linear, curvilinear, etc. regression curve: the graph of the regression function probability distributions: symmetrical, skewed etc. Regression models with more than one predictor variable Construction of Regression Models Selection of predictor variables Functional form of regression relation Scope of model Uses of regression analysis description control prediction overlap in practice Regeression and causality Use of Computers 2.3 Simple linear regression model with distribution of error terms unspecified Formal statement of model \\[Y_{i} = \\beta_{0} + \\beta_{1}X_{i} + \\varepsilon_{i}\\] where: \\(Y_{i}\\) is the value of th response variable in the ith trail \\(\\beta_{0}\\text{ and }\\beta_{1}\\) are paramters \\(X_{i}\\) is a known constant, namely, the value of the predictor variable in the ith trial \\(\\varepsilon_{i}\\) is a random error term mean \\(E(\\varepsilon_{i}) = 0\\) variance \\(\\sigma^{2}(\\varepsilon_{i}) = \\sigma^{2}\\) covariance \\(\\sigma(\\varepsilon_{i}, \\varepsilon_{j}) = 0\\), for all i, j; \\(i \\neq j\\) Important features of model \\(Y_{i}\\) contains two components: the constant term \\(\\beta_{0} + \\beta_{1}X_{i}\\) and the random term \\(\\varepsilon_{i}\\). Hence, \\(Y_{i}\\) is a random variable Since \\(E(\\varepsilon_{i}) = 0\\), \\(E(Y_{i}) = E(\\beta_{0} + \\beta_{1}X_{i} + \\varepsilon_{i}) = \\beta_{0} + \\beta_{1}X_{i} + E(\\varepsilon_{i}) = \\beta_{0} + \\beta_{1}X_{i}\\) The response \\(Y_{i}\\) in the ith trail exceeds or falls short of the value of the regssion fucntion by the error term amount \\(\\varepsilon_{i}\\) The erorr term \\(\\varepsilon_{i}\\) are assumed to have constant variance \\(\\sigma^{2}\\) and \\(\\sigma^{2}(Y_{i}) = \\sigma^{2}\\) The error terms are assumed to be uncorrelated, so are the responses \\(Y_{i}\\) and \\(Y_{j}\\) Meaning of regression paramters regrssion coefficients: the paramters \\(\\beta_{0}\\text{ and }\\beta_{1}\\) the slope of the regression line: \\(\\beta_{1}\\) Alternative versions of regression model \\[Y_{i} = \\beta_{0}X_{0} + \\beta_{1}X_{i} + \\varepsilon_{i}\\text{, where }X_{0} \\equiv 1\\] \\[Y_{i} = \\beta_{0}^{*} + \\beta_{1}(X_{i} - \\bar{X}) + \\varepsilon_{i}\\text{, where }\\beta_{0}^{*} = \\beta_{0} + \\beta_{1}\\bar{X}\\] 2.4 Data for regression analysis Observational Data Eperimental Data treatment experimental units Completely randomized design 2.5 Overview of steps in regression analysis 2.6 Estimation of regression function Methods of Least Squares To find estimates \\(b_{0}\\) and \\(b_{1}\\) for \\(\\beta_{0}\\) and \\(\\beta_{1}\\), respectively, for which Q is a minimum, where \\(Q = \\displaystyle\\sum_{i=1}^{n}(Y_{i} - \\beta_{0} - \\beta_{1}X_{i})^2\\). Least Squares Estimators \\(b_{0}\\) and \\(b_{1}\\) can be found in two ways: numerical search procedures analytical procedures \\[b_{1} = \\frac{\\sum(X_{i} - \\bar{X})(Y_{i} - \\bar{Y})}{\\sum(X_{i} - \\bar{X})^2}\\] \\[b_{0} = \\frac{1}{n}(\\sum Y_{i} - b_{1} \\sum X_{i}) = \\bar{Y} - b_{1}\\bar{X}\\] Proof The paritial derivatives are \\[\\frac{\\partial Q}{\\partial\\beta_{0}} = -2\\sum(Y_{i} - \\beta_{0} - \\beta_{1}X_{i})\\] \\[\\frac{\\partial Q}{\\partial\\beta_{1}} = -2\\sum X_{i}(Y_{i} - \\beta_{0} - \\beta_{1}X_{i})\\] We set them equal to zero, using \\(b_{0}\\) and \\(b_{1}\\) to denote the particular values of \\(b_{0}\\) and \\(b_{1}\\) that minimize Q: \\[-2\\sum(Y_{i} - \\beta_{0} - \\beta_{1}X_{i}) = 0\\] \\[-2\\sum X_{i}(Y_{i} - \\beta_{0} - \\beta_{1}X_{i}) = 0\\] Proerties of Least Squares Estimators Guass-Markov theorem: Under the conditions of regression model, the least squares estimators b0 and b1 are unbiased and have minimum variance among all unbiased linear estimators Point Esitmation of Mean Response estimate the regression function as follows: \\[\\hat{Y} = b_{0} + b_{1}X\\] Residuals residual: the differenc between the observed value \\(Y_{i}\\) and the corresponding fitted value \\(\\hat{Y_{i}}\\) \\[e_{i} = Y_{i} - \\hat{Y}_{i}\\] Properties of Fitted Regression Line \\(\\sum e_{i} = 0\\) \\(\\sum e_{i}^{2}\\) is a minimum \\(\\sum Y_{i} = \\sum \\hat{Y}_{i}\\) \\(\\sum X_{i} e_{i} = 0\\) \\(\\sum \\hat{Y}_{i}\\)ei = 0 the regression line always goes through the point \\((\\bar{X}, \\bar{Y})\\) 2.7 Estimation of Erro Terms Variance \\(\\sigma^{2}\\) Point Estimator of \\(\\sigma^{2}\\) Single population sum of squares: \\(\\displaystyle\\sum_{i=1}^{n}(Y_{i}-\\bar{Y})^2\\) degrees of freedom (df): n - 1, because one degree of freedom is lost by using \\(\\bar{Y}\\) as an estimate of the unknown population mean \\(\\mu\\) sample variance/mean square: \\(s^2 = \\frac{\\displaystyle\\sum(Y_{i}-\\bar{Y})^2}{n-1}\\) Regression model deviation/residual: \\(Y_{i} - \\hat{Y}_{i}\\) = ei error/residual sum of squares: \\(SSE = \\displaystyle\\sum_{i=1}^{n}(Y_{i} - \\hat{Y}_{i})^{2}\\) \\(SSE = \\displaystyle\\sum_{i=1}^{n}e_{i}^{2}\\) degrees of freedom: n - 2, because two degrees of freedom are lost due to estimating \\(\\beta_{0}\\) and \\(\\beta_{1}\\) to get \\(\\hat{Y}_{i}\\) MSE (error/residual mean square): \\(s^{2} = MSE = \\frac{SSE}{n-2}\\) \\(E(MSE) = \\sigma^2\\) (Note: \\(SSE/\\sigma^2 \\sim \\chi^2(n-2)\\)), so MSE is an unbiased estimator of \\(\\sigma^2\\) 2.8 Normal Error Regression Model Model same with simple linear regression model except it assumes that the error \\(\\varepsilon_{i}\\) are normally distributed Estimation of Parameters by Method of Maximum Likelihood Method of maximum likelihood chooses as estimates those values of the parameters that are most consistent with the sample data. A normal distribution with SD = 10, mean is unknown A random of sample n = 3 yields the results 250, 265 and 259 The likelihood value (L) is the product of the densities of the normal distribution If we assue μ = 230, L(μ = 230) = 0.279*10E-9 R code: prod(dnorm(c(250,265,259), 230, 10)) If we assue μ = 259, L(μ = 259) = 0.0000354 R code: prod(dnorm(c(250,265,259), 259, 10)) So, L(μ = 259) &gt; L(μ = 230) The method of maximum likelihood is to estimate prarametes to get maximum L. It can be shown that for a normal population, the maximum likelihood estimator of μ is the smaple mean For regression model, the likelihood function for n observations is \\[L(\\beta_{0}, \\beta_{1}, \\sigma^{2}) = \\displaystyle\\Pi_{i=1}^{n}\\frac{1}{(2\\pi\\sigma^{2})^{1/2}}exp[-\\frac{1}{2\\sigma^{2}}(Y_{i} - \\beta_{0} - \\beta_{1}X_{i})^{2}]\\] \\[= \\frac{1}{(2\\pi\\sigma^{2})^{1/2}}exp[-\\frac{1}{2\\sigma^{2}}\\displaystyle\\sum_{i=1}^{n}(Y_{i} - \\beta_{0} - \\beta_{1}X_{i})^{2}]\\] So we can get the maximu likelihood estimator: Parameter Maximum Likelihood Estimator \\(\\beta_0\\) \\(\\hat{\\beta_0} = b_0\\) \\(\\beta_1\\) \\(\\hat{\\beta_1} = b_1\\) \\(\\sigma^2\\) \\(\\hat{\\sigma^2} = \\frac{\\sum(Y_i-\\hat{Y_i})^2}{n}\\) The maximum likelihood estimator \\(\\sigma^2\\) is biased. The unbiased one is MSE or \\(s^2\\). \\[s^2 = MSE = \\frac{n}{n-2}\\hat{\\sigma^2}\\] "],
["inferences-in-regeression-and-correlation-analysis.html", "3 Inferences in Regeression and Correlation Analysis 3.1 Inferences Concerning \\(\\beta_{1}\\) 3.2 Inferences Concerning \\(\\beta_{0}\\) 3.3 Some Considerations on Making Inferences Concerning \\(\\beta_{0}\\) and \\(\\beta_{1}\\) 3.4 Interval Estimation of \\(E(Y_{h})\\) 3.5 Prediction of New Observation 3.6 Confidence Band for Regression Line 3.7 Analysis of Variance Approach 3.8 General Linear Test Approach 3.9 Descriptive Measures of Linear Association between X and Y 3.10 Considerations in Applying Regression Analysis 3.11 Normal Correlation Models 3.12 R code", " 3 Inferences in Regeression and Correlation Analysis 3.1 Inferences Concerning \\(\\beta_{1}\\) Sampling Distribution of b1 The sampling distribution of b1 refers to the different values of b1 that would be obtained with repeated sampling when the levels of the predictor variable X are held constant from sample to sample. For normal error regression model, the sample distributon of b1 is normal, with mean and variance: \\[E(b_1) = \\beta_{1}\\] \\[\\sigma^{2}(b_1) = \\frac{\\sigma^{2}}{\\sum(X_{i} - \\bar{X})^{2}}\\] Proof b1 as linear combination of the Yi \\[b1 = \\sum k_{i}Y_{i}\\text{ where }k_{i} = \\frac{X_{i} - \\bar{X}}{\\sum(X_{i} - \\bar{X})^{2}}\\] - Nomaily The \\(Y_{i}\\) are independently, normally distributed, so b1 are normally distributed. Mean \\[E(b_{1}) = E(\\sum k_{i}Y_{i}) = \\sum k_{i}E(Y_{i}) = \\sum k_{i}(\\beta_{0} + \\beta_{1}X_{i}) = \\beta_{1}\\] hint: \\[\\sum k_{i} = 0\\] \\[\\sum k_{i}X_{i} = 1\\] Variance \\[\\sigma^{2}(b_{1}) = \\sigma^{2}(\\sum k_{i}Y_{i}) = \\sum k_{i}^{2}\\sigma^{2}(Y_{i}) = \\sum k_{i}^{2}\\sigma^{2} = \\sigma^{2}\\frac{1}{\\sum (X_{i} - \\bar{X})^{2}}\\] Estimated Variance Replace the paramter \\(\\sigma^{2}\\) with MSE: \\[s^{2}(b_{1}) = \\frac{MSE}{\\sum(X_{i} - \\bar{X})^{2}}\\] Sampling Distribution of \\((b_{1} - \\beta_{1})/s(b_{1})\\) \\[(b_{1} - \\beta_{1})/\\sigma(b_{1}) \\sim N(0,1)\\] \\[(b_{1} - \\beta_{1})/s(b_{1}) \\sim t(n-2)\\] When a statistic is standardized but the denominator is an estimated standard deviation rather than the true standard deviation, it is called a studentized statistic. Comment \\[SSE/\\sigma^{2} \\sim \\chi^{2}(n - 2)\\] \\[(b_{1} - \\beta_{1})/s(b_{1}) \\sim \\frac{z}{\\sqrt{\\frac{\\chi^2(n-2)}{n-2}}} = t(n-2)\\] Confidence Interval for \\(\\beta_{1}\\) \\[b_{1} \\pm t(1-\\alpha/2; n-2)s(b_{1})\\text{ where }\\alpha\\text{ is significance level}\\] Tests concerning \\(\\beta_{1}\\) Since \\((b_{1} - \\beta_{1})/s(b_{1})\\) is ditributed as t with n - 2degrees of freedom, tests concerning \\(\\beta_{1}\\) can be set up in ordinary fashion using the t distribution. 3.2 Inferences Concerning \\(\\beta_{0}\\) The sampling distribution of \\(\\beta_{0}\\) is normal, with mean and variance: \\[E(b_{0}) = \\beta_{0}\\] \\[\\sigma^{2}(b_{0}) = \\sigma^{2}[\\frac{1}{n} + \\frac{\\bar{X}^{2}}{\\sum (X_{i} - \\bar{X})^{2}}]\\] \\[s^{2}(b_{0}) = MSE[\\frac{1}{n} + \\frac{\\bar{X}^{2}}{\\sum (X_{i} - \\bar{X})^{2}}]\\] \\[\\frac{b_{0} - \\beta_{0}}{s(b_{0})} \\sim t(n-2)\\] 3.3 Some Considerations on Making Inferences Concerning \\(\\beta_{0}\\) and \\(\\beta_{1}\\) Effects of Departures from Normality Interpretation of Confidence Coefficient and Risks of Errors Spacing of the X levels Power of Tests The power of this test is the probability that the decision rule will lead to conclusion \\(H_{a}\\) when \\(H_{a}\\) in fact holds. Specifically, the power is given by \\[Power = P(|t^{*}| &gt; t(1-\\alpha/2;n-2)|\\delta)\\] where, \\(H_{0}: \\beta_{1} = \\beta_{10}\\); \\(H_{a}: \\beta_{1} \\neq \\beta_{10}\\) \\(t^{*} = \\frac{b_{1} - \\beta_{10}}{s(b_{1})}\\) \\(\\delta\\) is the noncentrality measure, a measure of how far the true value of \\(\\beta_{1}\\) is from \\(\\beta_{10}\\). \\(\\delta = \\frac{\\mid\\beta_{1} - \\beta_{10}\\mid}{\\sigma(b_{1})}\\) 3.4 Interval Estimation of \\(E(Y_{h})\\) The mean response when \\(X = X_{h}\\) is denoted by \\(E(Y_{h})\\). The \\(E(Y_{h})\\) point estimator \\(\\hat{Y}_{h}\\) : \\[\\hat{Y}_{h} = b_{0} + b_{1}X_{h}\\] Sampling Distribution of \\(\\hat{Y}_{h}\\) For normal error regression model, the sampling distribution of \\(\\hat{Y}_{h}\\) is normal, with mean and variance: \\[E(\\hat{Y}_{h}) = E(Y_{h})\\] \\[\\sigma^{2}(\\hat{Y}_{h}) = \\sigma^{2}[\\frac{1}{n} + \\frac{(X_{h} - \\bar{X})^2}{\\sum(X_{i} - \\bar{X})^{2}}]\\] \\[s^{2}(\\hat{Y}_{h}) = MSE[\\frac{1}{n} + \\frac{(X_{h} - \\bar{X})^{2}}{\\sum (X_{i} - \\bar{X})^{2}}]\\] \\[\\frac{\\hat{Y}_{h} - E(Y_{h})}{s(\\hat{Y}_{h})} \\sim t(n-2)\\] 3.5 Prediction of New Observation We denote the level of X for the new trial as \\(X_{h}\\) and the new observation on Y as \\(Y_{h(new)}\\). In the former case, the estimation of \\(E(Y_{h})\\) is the mean of the distribution of Y; in the present case, we predict an individual outcome draw from the distribution of Y. Hence, two components of \\(\\sigma(pred)\\): The variance of the distribution of Y at \\(X = X_{h}\\), namely \\(\\sigma^{2}\\) The variance of the sampling distribution of \\(\\hat{Y}_h\\), namely \\(\\sigma^{2}(\\hat{Y}_h)\\) \\[\\sigma^{2}(pred) = \\sigma^{2}(Y_{h(new)} - \\hat{Y}_{h}) = \\sigma^{2} + \\sigma^{2}(\\hat{Y}_{h})\\] \\[s^{2}(pred) = MSE[1 + \\frac{1}{n} + \\frac{(X_{h} - \\bar{X})^{2}}{\\sum (X_{i} - \\bar{X})^{2}}]\\] 3.6 Confidence Band for Regression Line To obtain a confidence band for the entire for the entire regression line \\(E(Y) = \\beta_{0} + \\beta_{1}X\\). The Working-Hotellling 1 - \\(\\alpha\\) confidence band: \\[\\hat{Y}_{h} \\pm Ws(\\hat{Y}_{h})\\] where, \\[W^{2} = 2F(1-\\alpha; 2, n-2)\\] Since, we are doing all values of \\(X_{h}\\) at once, it will be wider at each \\(X_{h}\\) than CIs for individual \\(X_{h}\\). 3.7 Analysis of Variance Approach Partitioning of Total Sum of Squares \\[Y_{i} - \\bar{Y} = \\hat{Y}_{i} - \\bar{Y} + Y_{i} - \\hat{Y}_{i}\\] \\[\\sum (Y_{i} - \\bar{Y})^{2} = \\sum (\\hat{Y}_{i} - \\bar{Y})^{2} + \\sum (Y_{i} - \\hat{Y}_{i})^{2}\\] \\[SSTO = SSR + SSE\\] SSTO stands for total sum of squares, SSE stands for error sum of squares and SSR stands for regression sum of squares. Breakdown of Degrees of Freedom \\[n - 1 = 1 + (n - 2)\\] We have n-1 degrees of freedom associated with SSTO. SSE has n-2 degrees of freedom and SSR has 1 degree of freedom. Mean Squares A sum of squares divided by its associated degrees of freedom is called a mean square (MS) The mean squares are not additive: \\(\\frac{SSTO}{n-1} \\neq \\frac{SSR}{1} + \\frac{SSE}{n-2} = MSR + MSE\\) ANalysis Of VAriance Table (ANOVA table) ANOVA table: The breakdowns of the total sum of squares and associated degrees of freedom are displayed in the form of ANVOA. SSTOU: the total uncorrected sum of squares, \\(\\sum Y_i^2\\) SS: correction for the mean sum of squares, \\(n\\bar{Y}^2\\) SSTO = \\(\\sum (Y_i - \\bar{Y})^2 = \\sum Y_i^2 - n\\bar{Y}^2 = SSTOU + SS\\) Source of Variation SS df MS Regression \\(SSR = \\sum(\\hat{Y}_i - \\bar{Y})^2\\) 1 \\(MSR = \\frac{SSR}{1}\\) Error \\(SSE = \\sum(Y_i - \\hat{Y}_i)^2\\) n-2 \\(MSE = \\frac{SSE}{n-2}\\) Total \\(SSTO = \\sum(Y_i - \\bar{Y})^2\\) n-1 Correction for mean \\(SS \\text{(correction for mean)} = n\\bar{Y}^2\\) 1 Total, uncorrected \\(SSTOU = \\sum Y_i^2\\) n Expected Mean Squares \\[E(MSE) = \\sigma^2\\] \\[E(MSR) = \\sigma^2 + \\beta_1^2 \\sum (X_i - \\bar{X})^2\\] F test for \\(\\beta_1 = 0\\) versus \\(\\beta_1 \\neq 0\\) Test Statistic: \\(F^* = \\frac{MSR}{MSE} \\sim F(1,n-2)\\) 3.8 General Linear Test Approach Two models: \\(Y_i = \\beta_0 + \\beta_2X_i + \\varepsilon_i\\) (full model) \\(Y_i = \\beta_0 + \\varepsilon_i\\) (reduced model under H0) F-statistic: \\[F = \\frac{(SSE(R) - SSE(F))/(df_R - df_F)}{SSE(F)/df_F}\\] The general linear teest approach can be used for highly complex tests of linear statistical models, as well as for simple tests. The basic steps in summary form are: Fit the full model and obtain the error sum of squares SSE(F) Fit the reduced model under H0 and obtain the error sum of squares SSE(R) Use the test statistic and desicison rule 3.9 Descriptive Measures of Linear Association between X and Y Coefficient of Determination \\[R^2 = \\frac{SSR}{SSTO} = 1 - \\frac{SSE}{SSTO}, 0 \\leq R^2 \\leq 1\\] Limitations of \\(R^2\\) Tree common misunderstandings about \\(R^2\\) A high coefficient of determination indicates that useful predictions can be made. A high coefficient of determination indicates that the estimated regression line is a good fit. A coefficient of determination near zero indicates that X and Y are not related. Coefficient of Correlation \\[r = \\pm \\sqrt{R^2}, -1 \\leq r \\leq 1\\] 3.10 Considerations in Applying Regression Analysis make inferences the predictor variable itsef often has to be predicted the levels of the predictor variable that fall outside the range of observations \\(\\beta_1 \\neq 0\\) doesnot establish a cause-and-effect relation multiple testing observations on the predictor variable X are subject to measurement erros 3.11 Normal Correlation Models Distinction between Regression and Correlation Model Bivariate Normal Distribution Conditional Inferences Inferences on Correlation Coefficients Spearman Rank Correlation Coefficient 3.12 R code 3.12.1 Example data head(trees) ## Girth Height Volume ## 1 8.3 70 10.3 ## 2 8.6 65 10.3 ## 3 8.8 63 10.2 ## 4 10.5 72 16.4 ## 5 10.7 81 18.8 ## 6 10.8 83 19.7 X = trees$Volume ## 体积 Y = trees$Girth ## 直径 3.12.2 built-in function fit &lt;- lm(Y~X) summary(fit) ## ## Call: ## lm(formula = Y ~ X) ## ## Residuals: ## Min 1Q Median 3Q Max ## -1.2945 -0.5742 -0.1520 0.7131 1.5248 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 7.677857 0.308628 24.88 &lt;2e-16 *** ## X 0.184632 0.009016 20.48 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.8117 on 29 degrees of freedom ## Multiple R-squared: 0.9353, Adjusted R-squared: 0.9331 ## F-statistic: 419.4 on 1 and 29 DF, p-value: &lt; 2.2e-16 coefficients(fit) # model coefficients ## (Intercept) X ## 7.6778570 0.1846321 confint(fit, level=0.95) # CIs for model parameters ## 2.5 % 97.5 % ## (Intercept) 7.0466415 8.3090724 ## X 0.1661924 0.2030719 fitted(fit) # predicted values ## 1 2 3 4 5 6 7 ## 9.579568 9.579568 9.561105 10.705824 11.148941 11.315110 10.558118 ## 8 9 10 11 12 13 14 ## 11.038162 11.850543 11.352036 12.145955 11.555132 11.628985 11.610521 ## 15 16 17 18 19 20 21 ## 11.204331 11.776690 13.918423 12.736777 12.422903 12.275197 14.047666 ## 22 23 24 25 26 27 28 ## 13.530696 14.380003 14.749268 15.543186 17.906477 17.961867 18.441910 ## 29 30 31 ## 17.186412 17.094096 21.894531 residuals(fit) # residuals ## 1 2 3 4 5 6 ## -1.27956795 -0.97956795 -0.76110474 -0.20582396 -0.44894108 -0.51511000 ## 7 8 9 10 11 12 ## 0.44188174 -0.03816180 -0.75054318 -0.15203642 -0.84595459 -0.15513177 ## 13 14 15 16 17 18 ## -0.22898462 0.08947859 0.79566928 1.12330967 -1.01842306 0.56322259 ## 19 20 21 22 23 24 ## 1.27709721 1.52480292 -0.04766555 0.66930442 0.11999661 1.25073235 ## 25 26 27 28 29 30 ## 0.75681418 -0.60647711 -0.46186675 -0.54191030 0.81358820 0.90590427 ## 31 ## -1.29453117 anova(fit) # anova table ## Analysis of Variance Table ## ## Response: Y ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## X 1 276.328 276.328 419.36 &lt; 2.2e-16 *** ## Residuals 29 19.109 0.659 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 3.12.3 point estimator \\(b_0\\) and \\(b_1\\) n = nrow(trees) Xbar = mean(X) Ybar = mean(Y) b1 = sum((X - Xbar)*(Y - Ybar))/sum((X-Xbar)^2) b0 = Ybar - b1*Xbar b1;b0 ## [1] 0.1846321 ## [1] 7.677857 3.12.4 Residuals, SSE and MSE residual = Y - b1*X - b0 SSE = sum(residual^2) MSE = SSE/(n-2) SSE; MSE; sqrt(MSE) ## [1] 19.10893 ## [1] 0.6589286 ## [1] 0.8117442 3.12.5 sampling distribution of \\(b_1\\) and \\((b_1−\\beta_1)/s(b_1)\\) s = sqrt( MSE/sum((X - Xbar)^2)) t = b1 / s p = (1 - pt(t, n -2))*2 s; t; p ## [1] 0.009015995 ## [1] 20.47829 ## [1] 0 3.12.6 F test SSTO = var(Y) * (n-1) F = (SSTO - SSE)/((n-1) - (n-2)) / (SSE/(n-2)) F ## [1] 419.3603 3.12.7 \\(R^2\\) and r Rsq = 1 - SSE/SSTO r = b1/abs(b1) * sqrt(Rsq) Rsq; r; cor(X,Y) ## [1] 0.9353199 ## [1] 0.9671194 ## [1] 0.9671194 3.12.8 plot plot(X,Y) abline(b0,b1, col=&quot;red&quot;) "],
["diagnostics-and-remedial-measures.html", "4 Diagnostics and Remedial Measures", " 4 Diagnostics and Remedial Measures "],
["simultaneous-inferences-and-other-topics-in-regression-analysis.html", "5 Simultaneous Inferences and Other Topics in Regression Analysis", " 5 Simultaneous Inferences and Other Topics in Regression Analysis "],
["matrix-approach-to-simple-linear-regression-analysis.html", "6 Matrix Approach to Simple Linear Regression Analysis 6.1 Matrices 6.2 Matrix Addition and Subtraction 6.3 Matrix Multiplication 6.4 Special Types of Matrices 6.5 Linear Dependence and Rank of Matrix 6.6 Inverse of a Matrix 6.7 Some Basic Results for Matrics 6.8 Random Vectors and Matrices 6.9 Simple Linear Regression Model in Matrix Terms 6.10 Leasst Squares Estimation of Regression Parameters 6.11 Fitted Values and Residuals 6.12 Analysis of Variance Results 6.13 Inferences in Regeression Analysis 6.14 R code", " 6 Matrix Approach to Simple Linear Regression Analysis 6.1 Matrices Definition matrix elements dimension Notation: a boldface symbol, such as A, X or Z. Square Matrix: the number of rows equals the number of columns Vector column vector/vector: only one column matrix row vector Transpose: A’ (A prime) is the transpose of a matrix A Equality of Matrices: same dimension and all same corresponding elements design matrix 6.2 Matrix Addition and Subtraction same dimension the sum or difference of the corresponding elements of the two matrixs A + B = B + A 6.3 Matrix Multiplication Multiplication of a Matrix by a scalar a scalar is an ordinary number or a symbol representing a number Multiplication of a Matrix by a Matrix the product AB, we say that A is postmultiplied by B or B is premultiplied by A \\(AB \\neq BA\\) In general, if A has dimension r * c and B has dimension c * s, the product AB is a matrix of dimension r * s, which is \\[AB_{r \\times s} = \\begin{bmatrix}\\sum_{k=1}^{c} a_{ik}b_{kj}\\end{bmatrix}\\text{, where }i=1,...,r;j=1,...,s\\] Regression Examples \\[Y&#39;Y_{1 \\times 1} = \\begin{bmatrix} Y_{1} &amp; Y_{2} &amp; ... &amp; Y_{n} \\end{bmatrix}\\begin{bmatrix}Y_{1} \\\\ Y_{2} \\\\ ... \\\\ Y_{n} \\end{bmatrix} = Y_{1}^{2} + Y_{2}^{2} + ... + Y_{n}^{2} = \\sum Y_{i}^{2}\\] \\[X&#39;X_{2 \\times 2} = \\begin{bmatrix} 1 &amp; 1 &amp; ... &amp; 1 \\\\ X_{1} &amp; X_{2} &amp; ... &amp; X_{n} \\end{bmatrix}\\begin{bmatrix} 1 &amp; X_{1} \\\\ 1 &amp; X_{2} \\\\ ... \\\\ 1 &amp; X_{n} \\end{bmatrix} = \\begin{bmatrix} n &amp; \\sum X_{i} \\\\ \\sum X_{i} &amp; \\sum X_{i}^{2} \\end{bmatrix}\\] \\[X&#39;Y_{2 \\times 1} = \\begin{bmatrix} 1 &amp; 1 &amp; ... &amp; 1 \\\\ X_{1} &amp; X_{2} &amp; ... &amp; X_{n} \\end{bmatrix}\\begin{bmatrix} Y_{1} \\\\ Y_{2} \\\\ ... \\\\ Y_{n} \\end{bmatrix} = \\begin{bmatrix} \\sum Y_{i} \\\\ \\sum X_{i}Y_{i} \\end{bmatrix}\\] 6.4 Special Types of Matrices Symmetric Matrix: A = A’ Symmetric matrix is necessarily square premultiply a matrix by its transpose, say X’X is symmetric Diagonal Matrix: off-diagonal elements are all zeros Identity Matrix, denoted by I: a diagonal matrix whose elements on the main diagonal are all 1s. AI = IA = A, \\(A, I \\in \\mathbb{R}^{r \\times r}\\) Scalar Matrix: a diagonal matrix whose main-diagonal elements are the same, can be expressed as kI Vector and matrix with all elements unity a column vector with all elements 1 will be denoted by 1 a square matrix with all elements 1 will be denoted by J 1’1 = n 11’ = J Zero Vector: a vector containing only zeros, denoted by 0 6.5 Linear Dependence and Rank of Matrix Linear dependence We define the set of c column vectors \\(C_{1}, ..., C_{c}\\) in an r \\(\\times\\) c matrix to be linearly dependent if one vector can be expressed as a linear combination of others. If no vector in the set can be so expressed, we define the set of vectors to be linearly independent. In a more general, when c scalars \\(k_{1},...,k_{c}\\), not all zero, can be found such that: \\[k_{1}\\mathbf{C_{1}} + k_{2}\\mathbf{C_{2}} + ... + k_{c}\\mathbf{C_{c}} = \\mathbf{0}\\] where 0 denotes the zero column vector, the c column vectors are linearly dependent. If the only set of scalars for which the equality holds is \\(k_{1} = k_{2} = ... = k_{c} = 0\\), the set of c column vectors is linearly independent. Rank of Matrix: the maximum number of linearly independent columns in the matrix the rank of r \\(\\times\\) c matrix cannot exceed min(r, c) When a matrix is the product of two matrixs, its rank cannot exceed the smaller of the two ranks for the matrices being multiplied. 6.6 Inverse of a Matrix The inverse of a matrix \\(\\mathbf{A}\\) is another matrix, denoted by \\(\\mathbf{A^{-1}}\\), such that: \\[\\mathbf{A}^{-1}\\mathbf{A} = \\mathbf{AA}^{-1} = \\mathbf{I}\\] where I is the identity matrix. the inverse of a matrix is defined only for square matrix many square matrix do not have inverse the inverse of a square matrix, if exits, is unique Finding the inverse An inverse of a square \\(r \\times r\\) matrix exists if the rank of the matrix is r. Such a matrix is said to be nonsingular or of full rank. An \\(r \\times r\\) matrix with rank less than r is said to be singular or not of full rank, and does not have an inverse. The inverse of an \\(r \\times r\\) matrix of full rank also has rank r. If: \\[\\mathbf{A}_{2 \\times 2} = \\begin{bmatrix} a &amp; b \\\\ c &amp; d \\end{bmatrix}\\] then: \\[\\mathbf{A}_{2 \\times 2}^{-1} = \\begin{bmatrix} \\frac{d}{D} &amp; \\frac{-b}{D} \\\\ \\frac{-c}{D} &amp; \\frac{a}{D} \\end{bmatrix}\\] where: \\[D = ad - bc\\] D is called the determinant(行列式) of the matrix A. If A were singular, its determinant would equal zero and no inverse of A would exist. Regression Example \\[\\mathbf{X}&#39;\\mathbf{X}_{2 \\times 2} = \\begin{bmatrix} n &amp; \\sum X_{i} \\\\ \\sum X_{i} &amp; \\sum X_{i}^{2} \\end{bmatrix}\\] \\[ a = n, b = c = \\sum{X_{i}}, d = \\sum{X_{i}^{2}} \\] \\[ D = n\\sum{X_{i}^{2}} - (\\sum{X_{i}})^{2} = n\\sum{(X_{i} - \\bar{X})}^{2}\\] \\[(\\mathbf{X}&#39;\\mathbf{X})_{2 \\times 2}^{-1} = \\begin{bmatrix} \\frac{1}{n} + \\frac{\\bar{X}^2}{\\sum(X_{i} - \\bar{X})^{2}} &amp; \\frac{-\\bar{X}}{\\sum{(X_{i} - \\bar{X})^2}} \\\\ \\frac{-\\bar{X}}{\\sum{(X_{i} - \\bar{X})^2}} &amp; \\frac{1}{\\sum{(X_{i} - \\bar{X})^2}} \\end{bmatrix}\\] 6.7 Some Basic Results for Matrics A + B = B + A (A + B) + C = A + (B + C) (AB)C = A(BC) C(A + B) = CA + CB k(A + B) = kA + kB (A’)’ = A (A + B)‘= A’ + B’ (AB)‘= B’A’ (ABC)‘= C’B’A’ \\((AB)^{-1} = B^{-1}A^{-1}\\) \\((ABC)^{-1} = C^{-1}B^{-1}A^{-1}\\) \\((A^{-1})^{-1} = A\\) \\((A&#39;)^{-1} = (A^{-1})&#39;\\) 6.8 Random Vectors and Matrices A random vector or matrix contains elements that are random variables. Expectation of random vector or matrix: \\[\\mathbf{Y}_{3 \\times 1} = \\begin{bmatrix} Y_1 \\\\ Y_2 \\\\ Y_3 \\end{bmatrix}\\text{, and } \\mathbf{E(Y)}_{3 \\times 1 } = \\begin{bmatrix} E(Y_1) \\\\ E(Y_2) \\\\ E(Y_3) \\end{bmatrix}\\] Variance-Covariance Matrix of Random Vector \\[\\sigma^2(\\mathbf{Y})_{n \\times n} = \\begin{bmatrix} \\sigma^2(Y_1) &amp; \\sigma(Y_1,Y_2) &amp; ... &amp; \\sigma(Y_1, Y_n) \\\\ \\sigma(Y_2,Y_1) &amp; \\sigma^2(Y_2) &amp; ... &amp; \\sigma(Y_2, Y_n) \\\\ ... &amp; ... &amp; ... &amp; ...\\\\ \\sigma(Y_n, Y_1) &amp; \\sigma(Y_n, Y_2) &amp; ... &amp; \\sigma^2(Y_n) \\end{bmatrix}\\] which is a symmetric matrix. Some Basic Rules \\[\\mathbf{W} = \\mathbf{AY}\\], which W and Y are two random vectors and A is a constant matrix \\[E(\\mathbf{A}) = \\mathbf{A}\\] \\[E(\\mathbf{W}) = E(\\mathbf{AY}) = \\mathbf{A}E(\\mathbf{Y})\\] \\[\\sigma^2(\\mathbf{W}) = \\sigma^2(\\mathbf{AY}) = \\mathbf{A}\\sigma^2(\\mathbf{Y})\\mathbf{A&#39;}\\] Multivariate Normal Distribution 6.9 Simple Linear Regression Model in Matrix Terms The normal error regression model in matrix terms is: \\[\\underset{n \\times 1}{\\mathbf{Y}} = \\underset{n \\times 2}{\\mathbf{X}}\\underset{2 \\times 1}{\\boldsymbol{\\beta}} + \\underset{n \\times 1}{\\boldsymbol{\\varepsilon}}\\] , where \\[\\underset{n \\times 1}{\\mathbf{Y}} = \\begin{bmatrix}y_1 \\\\ y_2 \\\\ ... \\\\y_n\\end{bmatrix}, \\underset{n \\times 2}{\\mathbf{X}} = \\begin{bmatrix}1 &amp; x_1 \\\\ 1 &amp; x_2 \\\\ ... &amp; ... \\\\ 1 &amp; x_n \\end{bmatrix}, \\underset{2 \\times 1}{\\boldsymbol{\\beta}} = \\begin{bmatrix}\\beta_0 \\\\ \\beta_1 \\end{bmatrix} and \\underset{n \\times 1}{\\boldsymbol{\\varepsilon}} = \\begin{bmatrix}\\varepsilon_1 \\\\ \\varepsilon_2 \\\\ ... \\\\\\varepsilon_n\\end{bmatrix}\\] 6.10 Leasst Squares Estimation of Regression Parameters \\[\\mathbf{b} = (\\mathbf{X}&#39;\\mathbf{X})^{-1}\\mathbf{X}&#39;\\mathbf{Y},\\] where b is the vector of the least squares regression coefficients: \\[ \\mathbf{b} = \\begin{bmatrix} b_0 \\\\ b_1 \\end{bmatrix} \\] Note that, the inverse is only valid for square matrix and \\(\\mathbf{X}&#39;\\mathbf{X}\\) is definitely a square matrix. 6.11 Fitted Values and Residuals \\[\\hat{\\mathbf{Y}} = \\mathbf{X}\\mathbf{b} = \\mathbf{X}(\\mathbf{X}&#39;\\mathbf{X})^{-1}\\mathbf{X}&#39;\\mathbf{Y}\\] or \\[\\hat{\\mathbf{Y}} = \\mathbf{H}\\mathbf{Y}\\text{, with }\\underset{n \\times n}{\\mathbf{H}} = \\mathbf{X}(\\mathbf{X}&#39;\\mathbf{X})^{-1}\\mathbf{X}&#39;\\] The matrix H is called hat matrix. And it’s symmetric and has the special property (called idempotency): \\[\\mathbf{HH} = \\mathbf{H}\\] \\[\\mathbf{e} = \\mathbf{Y} - \\mathbf{\\hat{Y}} = \\mathbf{Y} - \\mathbf{HY} = (\\mathbf{I}-\\mathbf{H})\\mathbf{Y}\\] and the matrix \\(\\mathbf{I}-\\mathbf{H}\\), like the matrix \\(\\mathbf{H}\\), is symmetric and idempotent. The variance-covariance matrix of residuals e: \\[\\sigma^2(\\mathbf{e}) = \\sigma^2 \\times (\\mathbf{I}-\\mathbf{H})\\] and is estimated by: \\[s^2(\\mathbf{3}) = MSE \\times (\\mathbf{I}-\\mathbf{H})\\] Proof: \\[\\sigma^2(\\mathbf{e}) = \\sigma^2((\\mathbf{I}-\\mathbf{H})\\mathbf{Y}) = (\\mathbf{I}-\\mathbf{H})\\times \\sigma^2(\\mathbf{Y}) \\times (\\mathbf{I}-\\mathbf{H})&#39;\\] \\[\\sigma^2(\\mathbf{Y})= \\sigma^2 \\times \\mathbf{I}\\] \\[(\\mathbf{I}-\\mathbf{H})&#39; = (\\mathbf{I}-\\mathbf{H})\\] \\[(\\mathbf{I}-\\mathbf{H})(\\mathbf{I}-\\mathbf{H}) = \\mathbf{I}-\\mathbf{H}\\] \\[\\sigma^2(\\mathbf{e}) = \\sigma^2 \\times (\\mathbf{I}-\\mathbf{H})\\] 6.12 Analysis of Variance Results \\[SSTO = \\sum(Y_i - \\bar{Y})^2 = \\sum Y_i^2 - \\frac{(\\sum Y_i)^2}{n} = \\mathbf{Y}&#39;\\mathbf{Y} - (\\frac{1}{n})\\mathbf{Y}&#39;\\mathbf{JY}\\] \\[SSE = \\mathbf{e}&#39;\\mathbf{e} = (\\mathbf{Y}-\\mathbf{Xb})&#39;(\\mathbf{Y}-\\mathbf{Xb})=\\mathbf{Y}&#39;\\mathbf{Y} - \\mathbf{b}&#39;\\mathbf{X}&#39;\\mathbf{Y}\\] \\[SSR = \\mathbf{b}&#39;\\mathbf{X}&#39;\\mathbf{Y} - (\\frac{1}{n})\\mathbf{Y}&#39;\\mathbf{JY}\\] A quadratic form is defined as: \\[\\underset{1 \\times 1}{\\mathbf{Y}&#39;\\mathbf{AY}} = \\displaystyle\\sum_{i=1}^{n}\\displaystyle\\sum_{j=1}^{n}a_{ij}Y_iY_j\\text{, where }a_{ij} = a_{ji}\\] A is a symmetrc n by n matrix and is called the matrix of the quadratic form. So the sums of squares as quadratic forms as follows: \\[SSTO = \\mathbf{Y}&#39;[\\mathbf{I} - \\frac{1}{n}\\mathbf{J}]\\mathbf{Y}\\] \\[SSE = \\mathbf{Y}&#39;[\\mathbf{I} - \\mathbf{H}]\\mathbf{Y}\\] \\[SSR = \\mathbf{Y}&#39;[\\mathbf{H} - \\frac{1}{n}\\mathbf{J}]\\mathbf{Y} \\] Quadratic forms play an important role in statistics because all sums of squares in the analysis of variance for linear statistical models can be expressed as quadratic forms. 6.13 Inferences in Regeression Analysis Regression Coefficients The variance-covariance matrix of b: \\[\\sigma^2(\\mathbf{b}) = \\begin{bmatrix} \\sigma^2(b_0) &amp; \\sigma(b_0,b_1) \\\\ \\sigma(b_0,b_1) &amp; \\sigma^2(b_1) \\end{bmatrix} = \\sigma^2 \\times (\\mathbf{X}&#39;\\mathbf{X})^{-1} = \\begin{bmatrix} \\frac{\\sigma^2}{n} + \\frac{\\sigma^2\\bar{X}^2}{\\sum(X_i - \\bar{X})^2} &amp; \\frac{-\\bar{X}\\sigma^2}{\\sum(X_i - \\bar{X})^2} \\\\ \\frac{-\\bar{X}\\sigma^2}{\\sum(X_i - \\bar{X})^2} &amp; \\frac{\\sigma^2}{\\sum(X_i - \\bar{X})^2} \\end{bmatrix}\\] And the estimated variance-covariance matrix of b, denoted by \\(s^2(\\mathbf{b})\\): \\[s^2(\\mathbf{b}) = MSE \\times (\\mathbf{X}&#39;\\mathbf{X})^{-1}\\] Mean Response \\[s^2(\\hat{Y}_h) = MSE \\times (\\mathbf{X}_{h}&#39;(\\mathbf{X}&#39;\\mathbf{X})^{-1}\\mathbf{X}_h) = MSE \\times [\\frac{1}{n} + \\frac{(X_h - \\bar{X})^2}{\\sum(X_i - \\bar{X})^2}]\\] Prediction of new observation \\[s^2(pred) = MSE \\times (1+\\mathbf{X}_{h}&#39;(\\mathbf{X}&#39;\\mathbf{X})^{-1}\\mathbf{X}_h)\\] Proof: \\[\\sigma^2(\\mathbf{b}) = \\sigma^2(\\mathbf{(X&#39;X)^{-1}X&#39;Y}) = \\mathbf{(X&#39;X)^{-1}X&#39;}\\sigma^2(\\mathbf{Y})(\\mathbf{(X&#39;X)^{-1}X&#39;})&#39; = \\sigma^2 \\times (\\mathbf{X}&#39;\\mathbf{X})^{-1}\\] 6.14 R code example data head(trees) ## Girth Height Volume ## 1 8.3 70 10.3 ## 2 8.6 65 10.3 ## 3 8.8 63 10.2 ## 4 10.5 72 16.4 ## 5 10.7 81 18.8 ## 6 10.8 83 19.7 fit_lm = lm(trees$&quot;Girth&quot; ~ trees$&quot;Height&quot;) summary(fit_lm) ## ## Call: ## lm(formula = trees$Girth ~ trees$Height) ## ## Residuals: ## Min 1Q Median 3Q Max ## -4.2386 -1.9205 -0.0714 2.7450 4.5384 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -6.18839 5.96020 -1.038 0.30772 ## trees$Height 0.25575 0.07816 3.272 0.00276 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 2.728 on 29 degrees of freedom ## Multiple R-squared: 0.2697, Adjusted R-squared: 0.2445 ## F-statistic: 10.71 on 1 and 29 DF, p-value: 0.002758 fitted(fit_lm) ## 1 2 3 4 5 6 7 ## 11.713904 10.435169 9.923674 12.225399 14.527123 15.038617 10.690916 ## 8 9 10 11 12 13 14 ## 12.992640 14.271376 12.992640 14.015628 13.248387 13.248387 11.458157 ## 15 16 17 18 19 20 21 ## 12.992640 12.736893 15.550111 15.805858 11.969651 10.179422 13.759881 ## 22 23 24 25 26 27 28 ## 14.271376 12.736893 12.225399 13.504134 14.527123 14.782870 14.271376 ## 29 30 31 ## 14.271376 14.271376 16.061605 anova(fit_lm) ## Analysis of Variance Table ## ## Response: trees$Girth ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## trees$Height 1 79.665 79.665 10.707 0.002758 ** ## Residuals 29 215.772 7.440 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Least squaress estimation y = trees$&quot;Girth&quot; x = cbind(1,trees$&quot;Height&quot;) b = solve(t(x) %*% x) %*% t(x) %*% y b ## [,1] ## [1,] -6.1883945 ## [2,] 0.2557471 fitted values h = x %*% solve(t(x) %*% x) %*% t(x) dim(h); h[1:4,1:4] ## [1] 31 31 ## [,1] [,2] [,3] [,4] ## [1,] 0.06181471 0.08644526 0.09629747 0.05196250 ## [2,] 0.08644526 0.13160125 0.14966365 0.06838286 ## [3,] 0.09629747 0.14966365 0.17101012 0.07495100 ## [4,] 0.05196250 0.06838286 0.07495100 0.04539435 yhat = h %*% y head(yhat) ## [,1] ## [1,] 11.713904 ## [2,] 10.435169 ## [3,] 9.923674 ## [4,] 12.225399 ## [5,] 14.527123 ## [6,] 15.038617 sum of squares SSTO = t(y) %*% y - 1 / length(y) * t(y) %*% matrix(1, nrow=length(y), ncol=length(y)) %*% y SSTO ## [,1] ## [1,] 295.4374 "],
["multiple-linear-regression-i.html", "7 Multiple linear regression I 7.1 Multiple Regression Models 7.2 General Linear Regression Model in Matrix Terms 7.3 Estimation of Regression Coefficients 7.4 Fitted Values and Residuals 7.5 Analysis of Variance Results 7.6 Inferences about Regression Parameters", " 7 Multiple linear regression I 7.1 Multiple Regression Models 7.1.1 First-Order Model with Two Predictor Variables \\[Y_i = \\beta_0 + \\beta_1X_{i1} + \\beta_2X_{i2} + \\varepsilon_i\\] regression surface/response surface additive effects or not to interact partial regression coefficients 7.1.2 First-Order Model with More than Two Predictor Variables \\[Y_i = \\beta_0 + \\beta_1X_{i1} + \\beta_2X_{i2} + ... + \\beta_{p-1}X_{i,p-1}+\\varepsilon_i\\] \\[Y_i = \\beta_0 + \\displaystyle\\sum_{k=1}^{p-1}\\beta_{k}X_{ik} + \\varepsilon_i\\] \\[Y_i = \\displaystyle\\sum_{k=0}^{p-1}\\beta_{k}X_{ik} + \\varepsilon_i,\\text{ where }X_{i0} \\equiv 1\\] hyperplane: the response function, which is a plane in more than two dimensions. 7.1.3 General Linear Regression Model \\[Y_i = \\beta_0 + \\beta_1X_{i1} + \\beta_2X_{i2} + ... + \\beta_{p-1}X_{i,p-1}+\\varepsilon_i\\] where: \\(\\beta_0, \\beta_2, ..., \\beta_{p-1}\\) are parameters \\(X_{i1}, ..., X_{i,p-1}\\) are known constants \\(\\varepsilon_i\\) are independent \\(N(0,\\sigma^2)\\) \\(i = 1, ..., n\\) p-1 predictor variables qualitative predictor variables polynomia regression: \\[Y_i = \\beta_0 + \\beta_1X_{i1} + \\beta_2X_{i1}^2 + \\varepsilon_i\\] can be written as \\[Y_i = \\beta_0 + \\beta_1X_{i1} + \\beta_2X_{i2} + \\varepsilon_i\\text{ if }X_{i2} = X_i^2\\] transformed variables: \\[logY_i = \\beta_0 + \\beta_1X_{i1} + \\beta_2X_{i2} + \\varepsilon_i\\] can be treated as a general linear regression model: \\[Y_i^{&#39;} = \\beta_0 + \\beta_1X_{i1} + \\beta_2X_{i2} + \\varepsilon_i\\text{ if }Y_i^{&#39;}=logY_i\\] interaction effects \\[Y_i = \\beta_0 + \\beta_1X_{i1} + \\beta_2X_{i2} + \\beta_3X_{i1}X_{i2} + \\varepsilon_i\\] can be written as follows: \\[Y_i = \\beta_0 + \\beta_1X_{i1} + \\beta_2X_{i2} + \\beta_3X_{i3} + \\varepsilon_i\\text{ let }X_{i3} = X_{i1}X_{i2}\\] combination of cases \\[Y_i = \\beta_0 + \\beta_1X_{i1} + \\beta_2X_{i1}^2 + \\beta_3X_{i2} + \\beta_4X_{i2}^2 + \\beta_5X_{i1}X_{i2} + \\varepsilon_i\\] can be written as \\[Y_i = \\beta_0 + \\beta_1Z_{i1} + \\beta_2Z_{i2} + \\beta_3Z_{i3} + \\beta_4Z_{i4} + \\beta_5Z_{i5} + \\varepsilon_i\\] , where we define \\(Z_{i1}=X_{i1}, Z_{i2}=X_{i1}^2, Z_{i3}=X_{i2}, Z_{i4}=X_{i2}^2\\text{ and }Z_{i5}=X_{i1}X_{i2}\\) meaning of linear model: refers to linear in the parameters 7.2 General Linear Regression Model in Matrix Terms \\[\\underset{n \\times 1}{\\mathbf{Y}} = \\underset{n \\times p}{\\mathbf{X}}\\underset{p \\times 1}{\\boldsymbol{\\beta}} + \\underset{n \\times 1}{\\boldsymbol{\\varepsilon}}\\] , where \\[\\underset{n \\times 1}{\\mathbf{Y}} = \\begin{bmatrix}y_1 \\\\ y_2 \\\\ ... \\\\y_n\\end{bmatrix}, \\underset{n \\times p}{\\mathbf{X}} = \\begin{bmatrix}1 &amp; x_{11} &amp; x_{12} &amp; ... &amp; x_{1,p-1} \\\\ 1 &amp; x_{21} &amp; x_{22} &amp; ... &amp; x_{2,p-1} \\\\ ... &amp; ... &amp; ...&amp; ... &amp; ... \\\\ 1 &amp; x_{n1} &amp; x_{n2} &amp; ... &amp; x_{n,p-1}\\end{bmatrix}, \\underset{p \\times 1}{\\boldsymbol{\\beta}} = \\begin{bmatrix}\\beta_0 \\\\ \\beta_1 \\\\ ... \\\\ \\beta_{p-1} \\end{bmatrix} and \\underset{n \\times 1}{\\boldsymbol{\\varepsilon}} = \\begin{bmatrix}\\varepsilon_1 \\\\ \\varepsilon_2 \\\\ ... \\\\\\varepsilon_n\\end{bmatrix}\\] \\(\\mathbf{Y}\\) is a vector of responses \\(\\mathbf{\\beta}\\) is a vector of parameters \\(\\mathbf{X}\\) is a matrix of constants \\(\\boldsymbol{\\varepsilon}\\) is a vector of independent normal random variables expectation \\(E(\\boldsymbol{\\varepsilon}) = \\mathbf{0}\\) variane-covariance matrix: \\(\\sigma^2(\\boldsymbol{\\varepsilon}) = \\sigma^2\\mathbf{I}\\) Consequently, \\(E(\\mathbf{Y}) = \\boldsymbol{X\\beta}\\) and \\(\\sigma^2(\\boldsymbol{Y}) = \\sigma^2\\mathbf{I}\\) 7.3 Estimation of Regression Coefficients The least squares for general linear regression model: \\[Q = \\displaystyle\\sum_{i=1}^{n}(Y_i - \\beta_{0} - \\beta_1X_{i1} - ... - \\beta_{p-1}X_{i,p-1})^2\\] The least squares estimators are to minimize Q. \\[\\mathbf{X}&#39;\\mathbf{Xb} = \\mathbf{X}&#39;\\mathbf{Y}\\text{ and }\\mathbf{b} = (\\mathbf{X}&#39;\\mathbf{X})^{-1}\\mathbf{X}&#39;\\mathbf{Y}\\] The method of maximum likelihood leads to the same estimators for normal error regression model, which is obtained by maximizing this likelihood function: \\[L(\\boldsymbol{\\beta},\\sigma^2) = \\frac{1}{(2\\pi\\sigma^2)^{n/2}}exp[-\\frac{1}{2\\sigma^2}\\displaystyle\\sum_{i=1}^{n}(Y_i - \\beta_0 - \\beta_1X_{i1} - ... - \\beta_{p-1}X_{i,p-1})^2] \\] 7.4 Fitted Values and Residuals \\[\\hat{\\mathbf{Y}} = \\mathbf{X}\\mathbf{b} = \\mathbf{X}(\\mathbf{X}&#39;\\mathbf{X})^{-1}\\mathbf{X}&#39;\\mathbf{Y}\\] or \\[\\hat{\\mathbf{Y}} = \\mathbf{H}\\mathbf{Y}\\text{, with }\\underset{n \\times n}{\\mathbf{H}} = \\mathbf{X}(\\mathbf{X}&#39;\\mathbf{X})^{-1}\\mathbf{X}&#39;\\] The matrix H is called hat matrix. And it’s symmetric and has the special property (called idempotency): \\[\\mathbf{HH} = \\mathbf{H}\\] \\[\\mathbf{e} = \\mathbf{Y} - \\mathbf{\\hat{Y}} = \\mathbf{Y} - \\mathbf{HY} = (\\mathbf{I}-\\mathbf{H})\\mathbf{Y}\\] and the matrix \\(\\mathbf{I}-\\mathbf{H}\\), like the matrix \\(\\mathbf{H}\\), is symmetric and idempotent. The variance-covariance matrix of residuals e: \\[\\sigma^2(\\mathbf{e}) = \\sigma^2 \\times (\\mathbf{I}-\\mathbf{H})\\] and is estimated by: \\[s^2(\\mathbf{3}) = MSE \\times (\\mathbf{I}-\\mathbf{H})\\] 7.5 Analysis of Variance Results \\[SSTO = \\mathbf{Y}&#39;[\\mathbf{I} - \\frac{1}{n}\\mathbf{J}]\\mathbf{Y}\\] \\[SSE = \\mathbf{Y}&#39;[\\mathbf{I} - \\mathbf{H}]\\mathbf{Y}\\] \\[SSR = \\mathbf{Y}&#39;[\\mathbf{H} - \\frac{1}{n}\\mathbf{J}]\\mathbf{Y}\\] \\[MSR = \\frac{SSR}{p-1}\\] \\[MSE=\\frac{SSE}{n-p}\\] Source of Variation SS df MS Regression \\(SSR = \\mathbf{Y}&#39;[\\mathbf{H} - \\frac{1}{n}\\mathbf{J}]\\mathbf{Y}\\) p-1 \\(MSR = \\frac{SSR}{p-1}\\) Error \\(SSE = \\mathbf{Y}&#39;[\\mathbf{I} - \\mathbf{H}]\\mathbf{Y}\\) n-p \\(MSE = \\frac{SSE}{n-p}\\) Total \\(SSTO = \\mathbf{Y}&#39;[\\mathbf{I} - \\frac{1}{n}\\mathbf{J}]\\mathbf{Y}\\) n-1 F Test for Regression Relation \\[H_0: \\beta_1=\\beta_2=...=\\beta_{p-1}=0\\] \\[H_1:\\text{ not all }\\beta_{k}(k=1....p-1)\\] equal zero \\[F^* = \\frac{MSR}{MSE} \\sim F(p-1,n-p)\\] Coefficient of Multiple Determination \\[R^2 = \\frac{SSR}{SSTO} = 1 - \\frac{SSE}{SSTO}\\] Adding more X variables to the regression model can only increase \\(R^2\\) and never reduce it. The adjusted coefficient of multiple determination, denoted by \\(R^2_a\\): \\[R^2_a = 1-\\frac{\\frac{SSE}{n-p}}{\\frac{SSTO}{n-1}} = 1-(\\frac{n-1}{n-p})\\frac{SSE}{SSTO}\\] Coefficient of Multiple Correlation \\[R = \\sqrt{R^2}\\] 7.6 Inferences about Regression Parameters \\[E(\\mathbf{b}) = \\boldsymbol{\\beta}\\] \\[\\sigma^2(\\mathbf{b}) = \\sigma^2 \\times (\\mathbf{X}&#39;\\mathbf{X})^{-1} \\] \\[s^2(\\mathbf{b}) = MSE \\times (\\mathbf{X}&#39;\\mathbf{X})^{-1}\\] Interval Estimation of \\(\\beta_k\\) \\[\\frac{b_k - \\beta_k}{s(b_k)} \\sim t(n-p), k = 0,1, ...., p-1\\] \\[b_k \\pm t(1-\\frac{\\alpha}{2};n-p)s(b_k)\\] Tests for \\(\\beta_k\\) \\[H_0: \\beta_k = 0\\] \\[H_1: \\beta_k \\neq 0\\] \\[t^* = \\frac{b_k}{s(b_k)}\\] joint inferences The Bonferroni joint confidence intervals can be used to estimate several regression coefficients simultaneously. If g parameters are to be estimated jointly , then \\[b_k \\pm t(1-\\frac{\\alpha}{2g};n-p)s(b_k)\\] "],
["multiple-regression-ii.html", "8 Multiple Regression II 8.1 Extra Sums of Squares", " 8 Multiple Regression II 8.1 Extra Sums of Squares Defination An extra sum of squares measures: the marginal reduction in the error sum of squares (SSE) the marginal increase in the regression sum of squares (SSR) when one or several preictor variables are added to the regression model, given that other predictor variables are already in the model Example: SSR(X1|X2) = SSR(X1,X2) - SSR(X2) = SSSE(X2) - SSE(X1,X2) Decomposition of SSR into Extra Sums of Squares SSTO = SSR(X1) + SSE(X1) = SSR(X1) + SSR(X2|X1) + SSE(X1,X2) SSR(X1,X2,X3) = SSR(X1) + SSR(X2|X1) + SSR(X3|X1,X2) = SSR(X1) + SSR(X2,X3|X1) ANOVA Table Containing Decomposition of SSR "]
]
